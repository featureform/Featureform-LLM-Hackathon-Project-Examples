{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423f0449",
   "metadata": {},
   "source": [
    "# Featureform MLOps Podcast Chatbot\n",
    "\n",
    "This is an example of building a chatbot that contextualized with statements from the MLOps Weekly Podcast\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* Python 3.7+\n",
    "* `.env` file with one or both sets of credentials (visit [Pinecone](https://www.pinecone.io/) and/or [Weaviate](https://weaviate.io/) for instructions on creating an account and getting credentials):\n",
    "```\n",
    "# Pinecone\n",
    "\n",
    "PINECONE_PROJECT_ID=\n",
    "PINECONE_ENVIRONMENT=\n",
    "PINECONE_API_KEY=\n",
    "\n",
    "# Weaviate\n",
    "\n",
    "WEAVIATE_URL=\n",
    "WEAVIATE_API_KEY=\n",
    "\n",
    "# OpenAI\n",
    "\n",
    "You'll need to set your OpenAI key towards the bottom of this example, you'll also want to install the openai PyPI library using pip.\n",
    "```\n",
    "* [`python-dotenv 1.0.0`](https://pypi.org/project/python-dotenv/)\n",
    "* [Topic Labeled News Dataset](https://www.kaggle.com/datasets/kotartemiy/topic-labeled-news-dataset)\n",
    "* Featureform installed:\n",
    "```shell\n",
    "pip install featureform\n",
    "```\n",
    "* Hugging Face [`sentence-transformers`](https://huggingface.co/sentence-transformers) installed:\n",
    "```\n",
    "pip install sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf4382",
   "metadata": {},
   "source": [
    "## Step  1. Register Source\n",
    "\n",
    "`data/files` is a directory of CSV files, which use `;` as a delimiter and hold transcripts of recent episodes of the MLOps podcast. Each row is a comment made by a speaker and has the following columns:\n",
    "\n",
    "* Speaker\n",
    "* Start time\n",
    "* End time\n",
    "* Duration\n",
    "* Text\n",
    "* filename\n",
    "\n",
    "We'll register the entire directory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845783c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import featureform as ff\n",
    "from featureform import local\n",
    "\n",
    "client = ff.Client(local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a85622",
   "metadata": {},
   "source": [
    "**NOTE:** We'll create an instance of the client to register resources as we define them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288939b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = local.register_directory(\n",
    "    name=\"mlops-episodes\",\n",
    "    path=\"data/files\",\n",
    "    description=\"Transcripts from recent MLOps episodes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0f7fb",
   "metadata": {},
   "source": [
    "## Step 2. Transform Transcripts\n",
    "\n",
    "When registering a directory, files are converted into a table with columns `\"filename\"` and `\"body\"`. This is helpful for avoiding the situation where we need to register many files; however, in our case, we'll need to process this table to get it ready for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@local.df_transformation(inputs=[episodes], name=\"process_episode_files\")\n",
    "def process_episode_files(dir_df):\n",
    "    from io import StringIO\n",
    "    import pandas as pd\n",
    "\n",
    "    episode_dfs = []\n",
    "    for i, row in dir_df.iterrows():\n",
    "        csv_str = StringIO(row[1])\n",
    "        r_df = pd.read_csv(csv_str, sep=\";\")\n",
    "        r_df[\"filename\"] = row[0]\n",
    "        episode_dfs.append(r_df)\n",
    "\n",
    "    return pd.concat(episode_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f7c23",
   "metadata": {},
   "source": [
    "We can verify this worked as we expected by serving this source as a dataframe and inspecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.dataframe(process_episode_files)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12710fa4",
   "metadata": {},
   "source": [
    "## Step 3. Entity ID Transformation\n",
    "\n",
    "For our purposes, we'll need a unique identifier for each speakers' comments, so we'll choose `\"Speaker\"`, `\"Start time\"` and `\"filename\"` to create a new column, `\"PK\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "@local.df_transformation(inputs=[process_episode_files])\n",
    "def speaker_primary_key(episodes_df):\n",
    "    episodes_df[\"PK\"] = episodes_df.apply(lambda row: f\"{row['Speaker']}_{row['Start time']}_{row['filename']}\", axis=1)\n",
    "    \n",
    "    return episodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2648abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.dataframe(speaker_primary_key)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c9e07",
   "metadata": {},
   "source": [
    "## Step 4. Embeddings Transformation\n",
    "\n",
    "We'll use [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to create embeddings for each speakers' comments. When we register an entity and associate a feature with this entity, this transformation will be materialized and the embeddings will be persisted in a Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72eb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@local.df_transformation(inputs=[speaker_primary_key])\n",
    "def vectorize_comments(episodes_df):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(episodes_df[\"Text\"].tolist())\n",
    "    episodes_df[\"Vector\"] = embeddings.tolist()\n",
    "    \n",
    "    return episodes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb6af7",
   "metadata": {},
   "source": [
    "## Step 5. Register Pinecone\n",
    "\n",
    "We'll be using Pinecone for this example, but you can also choose to use Weaviate.\n",
    "\n",
    "This step assumes you have a `.env` file with your Pinecone credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "pinecone = ff.register_pinecone(\n",
    "    name=\"pinecone\",\n",
    "    project_id=os.getenv(\"PINECONE_PROJECT_ID\", \"\"),\n",
    "    environment=os.getenv(\"PINECONE_ENVIRONMENT\", \"\"),\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\", \"\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0308b",
   "metadata": {},
   "source": [
    "## Step 6. Register Entity, Features, and Embeddings and write them to Vector DB.\n",
    "\n",
    "We'll now register an entity and a feature, which will kick off the materialization process.\n",
    "\n",
    "**NOTE:**\n",
    "This may take some time to complete. See the progress bar for status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef327c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ff.entity\n",
    "class Speaker:\n",
    "    comment_embeddings = ff.Embedding(\n",
    "        vectorize_comments[[\"PK\", \"Vector\"]],\n",
    "        dims=384,\n",
    "        vector_db=pinecone,\n",
    "        description=\"Embeddings created from speakers' comments in episodes\",\n",
    "        variant=\"v1\"\n",
    "    )\n",
    "    comments = ff.Feature(\n",
    "        speaker_primary_key[[\"PK\", \"Text\"]],\n",
    "        type=ff.String,\n",
    "        description=\"Speakers' original comments\",\n",
    "        variant=\"v1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf3e4c",
   "metadata": {},
   "source": [
    "## Step 7. Register On-Demand Features to Retrieve Relevent Context\n",
    "\n",
    "We'll want to query the embeddings we created and then fetch their related docs and we can do so using Featureform's on-demand feature decorator. This creates a feature that's calculated on the client at serving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9fb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ff.ondemand_feature()\n",
    "def relevent_comments(client, params, entity):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    search_vector = model.encode(params[\"query\"])\n",
    "    res = client.nearest(\"comment_embeddings\", \"v1\", search_vector, k=params[2])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ff.ondemand_feature()\n",
    "def contextualized_prompt(client, params, entity):\n",
    "    pks = client.features([relevent_comments], {}, params=params)\n",
    "    prompt = \"Use the following snippets from our podcast to answer the following question\\n\"\n",
    "\n",
    "    res = client.nearest(\"comment_embeddings\", \"v1\", search_vector, k=params[2])\n",
    "    for pk in pks:\n",
    "        prompt += \"```\"\n",
    "        prompt += client.features([(\"comments\", \"v1\")], {\"PK\": pk})[0]\n",
    "        prompt += \"```\\n\"\n",
    "    prompt += \"Question: \"\n",
    "    prompt += params[\"query\"]\n",
    "    prompt += \"?\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98708b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0be59",
   "metadata": {},
   "source": [
    "# Finally we can feed our prompt into OpenAI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = client.features([contextualized_prompt], {}, params={\"query\": \"What should I know about MLOps for Enterprise\"})\n",
    "import openai\n",
    "openai.organization = \"\"\n",
    "openai.apikey = \"\"\n",
    "print(openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prompt,\n",
    ")[\"choices\"][0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
