Number;Speaker;Start time;End time;Duration;Text
0;Simba Khadder;00:00:05.470;00:00:23.120;00:00:17.650;"Hi, I'm Simba Khadder, and you're listening to the MLOps weekly podcast. Today I'm chatting with Atindriyo Sanyal, one of the founders of Galileo, Data Quality Platform. Prior to Galileo, he architected the Michelangelo, a feature store at Uber and worked on the ML platform Powering Siri. Atin, it's great to have you on the show today."
1;Atindriyo Sanyal;00:00:23.260;00:00:25.630;00:00:02.370;"Thank you for having me. It's great to be here, Simba."
2;Simba Khadder;00:00:25.750;00:00:30.760;00:00:05.010;"So I like to start with the question of what does MLOps mean to you?"
3;Atindriyo Sanyal;00:00:30.900;00:01:13.160;00:00:42.260;"That's a good question. MLOps, I think to me, it's really about bringing the discipline of DevOps in application development, which includes software design, unit integration, testing, the whole gamut of CI CD, including monitoring and observability, but bringing that to machine learning models, because models are simply software artifacts, just like libraries and APIs and services. The same principles sort of apply to automating the lifecycle of ML models right from the pre training phase, including feature engineering, all the way to deployment and monitoring. So in a nutshell, it's really about applying software engineering principles, but for models."
4;Simba Khadder;00:01:13.310;00:01:35.570;00:00:22.260;"So I love some ops analogy. It's a commonly used one. And I think I still kind of feel like everyone's still fleshing out what it really means, like what should be the same and what should be different. So you talk a lot about the parallels of what should be the same, but what is the difference? Almost like, why does MLOps have to exist? Can we just do everything in MLOps with DevOps tools? Or what does MLOps have that's unique?"
5;Atindriyo Sanyal;00:01:35.760;00:02:45.850;00:01:10.090;"Yes, that's a good question. So the way I see it, there are some nuances in machine learning, which makes MLOps a little bit different from traditional DevOps. And that primarily has to do with the APIs and the libraries that you sort of expose as part of your MLOps platform to be able to build models. Overall, it is similar in that even model code is just like application code, except that models are architected in a slightly different way than traditional software applications. So the differences are mostly in the APIs that an MLOps platform would expose to a data scientist. And a good MLOps platform, to that extent would essentially include the right abstractions, where you abstract away the complex feature engineering, for example, or deploying the models at scale while you surface the right endpoints. And the APIs to data scientists to be able to build custom ML pipelines that can sort of cater to their ML needs. And that, I think, in a nutshell, is sort of the difference between MLOps and DevOps. Got it."
6;Simba Khadder;00:02:45.970;00:03:04.070;00:00:18.100;"And on a similar thread, you've seen things from Apple at Siri and you've seen Uber, and now you're kind of going on working on Galileo and I guess, what's the North Star for MLOps, in your point of view? And then what does the perfect MLOps workflow look like, especially from a data scientist's point of view?"
7;Atindriyo Sanyal;00:03:04.230;00:04:23.700;00:01:19.470;"That's a good question. I think perfect MLOps workflow is really about, as I said, automating and abstracting away the right parts of the machine learning workflow while giving the flexibility and customisability to certain other parts. For example, a good MLOps workflow would automate the management of features but give you the right API's to design custom sort of transformation pipelines for your model. A good MLOps workflow would automate the deployment of models at scale and provide centralized monitoring and observability of all kinds of metrics. There's system level metrics, memory CPU, there's model level metrics like feature drift and prediction drift. So a good ML Op system, it's really not about click button ML, but rather you want to give modular APIs to data scientists so they can create ML pipelines which are custom and powerful and cater to their needs while giving the data and the features at their fingertips without them having to worry about how the feature values are kept up to date, how they're materialized. And once the models in production, you really want to give them tight SLAs and a robust alerting system so that they know when the models have gone wrong in their predictions."
8;Atindriyo Sanyal;00:04:23.790;00:04:33.360;00:00:09.570;"And you really want to tighten the loop on model downtimes and the way you can sort of retrain your model quickly and push it back into production."
9;Simba Khadder;00:04:33.510;00:04:51.770;00:00:18.260;"Where this experimentation fit into all this? Because a lot of what you've talked about is very oriented towards getting things into production. And I guess experimentation is obviously a really big part of machine learning that's I think unique to it compared to other disciplines of engineering or software engineering in particular. Where did experimentation fit into mob?"
10;Atindriyo Sanyal;00:04:51.900;00:06:05.630;00:01:13.730;"Yes, that's a good question. Experimentation sort of falls. So if you were to draw the ML lifecycle on the left hand side, you have the pretraining phase where you're choosing the right features for your models, then you have training and validation and evaluation and finally you do testing and you deploy your model. So experimentation is kind of sprinkled across the workflow at each point. It's almost like a sort of check to make sure that the model is performing per expectations at each point of the workflow. But it focuses a little more towards the left, where you're essentially choosing the data that you are fitting onto your model. You're choosing the right evaluation and test sets to test your model against. And then there's also, of course hyper parameters where you want to choose the right hyper parameters to get the best model for your data. So a lot of that has to do with experimentation. And today in a lot of the AutoML platforms, experimentation is typically done by spanning out different training jobs and really just choosing the one which gave the best result on a held out test set, which is sort of the sacrosanct data set."
11;Atindriyo Sanyal;00:06:05.790;00:06:18.110;00:00:12.320;"So experimentation is super critical to churning out a really good model, and it is sort of sprinkled across the different parts of the work flow, but it's primarily sort of left leaning."
12;Simba Khadder;00:06:18.300;00:06:59.900;00:00:41.600;"Yeah, it's super interesting how, like, everyone's mental map of the workflows is very different because I like the idea of this kind of flow from almost like data to production and monitoring and experimentation is almost like a layer that sits above all of it. And you've seen an infrastructure, both an Apple and an Uber. And what I've seen, it seems like the big companies actually have a much harder time building ML platforms. It's because there's so much bigger. We're spanning so many different use cases where Uber maybe was able to be more opinionated, and it seems like they were able to. But I'm curious. You've seen it on the inside of both places. What would you say the big differences are between how Apple and Uber built their machine learning infrastructure?"
13;Atindriyo Sanyal;00:07:00.050;00:08:12.070;00:01:12.020;"Yeah, both companies have published very interesting and innovative research in the machine learning space in the last two to three years. Although at Apple, the ML infrastructure is very decentralized. For example, the Maps team has their own internal ML stack, and so does the series team. And there's very little sharing. And that's just because of the way Apple at least has grown in its software services field in the last ten odd years. Even the research teams over there are pretty siloed. But Apple is now slowly trying to bring all the ML services together under a single umbrella. But given how big the company is, it's a very long process. Uber, on the other hand, is very interesting because they started with a centralized model of right from day zero. And I think that kind of worked in their favor because technologies like the concept of feature stores was evangelized at the company due to a company wide need for organizing and centralizing data for all their machine learning models. But that said, I think the scale challenges at both companies are very similar. Hence, the entire gamut of technologies that they use are fairly similar."
14;Atindriyo Sanyal;00:08:12.130;00:08:44.480;00:00:32.350;"There's a good amount of overlap. It's also very good to see Apple contributing a lot to open source. More recently, in the past systems like Spark, over the last two, three years, Apple has made significant contributions. Uber, on the other hand, has always been very actively involved in the open source community. A lot of key open source technologies came out of the MichaelAngelo team at Uber, for example, with Horoward and Ludwig, and these technologies have been adopted right across the industry. So I would say those are the key differences between Apple and Uber."
15;Simba Khadder;00:08:44.630;00:09:32.300;00:00:47.670;"Yeah, it makes a lot of sense. It's actually a very succinct way to put it. When we go and talk to large banks, for example, we find that a lot of times they have very decentralized infrastructure. And it's almost because when they started to really invest in their infrastructure, it was kind of very had many different teams, so they kind of had to be decentralized. Where I feel like Uber and some of these other companies are in the sweet spot of size, where they could invest in infrastructure, but they could also just do one centralized approach with the decentralized ones we've talked about like Apple and Siri, which is voice and it's a whole different kind of tablet data. Did the platform look similar? Like was there kind of the same idea of like a feature store of monitoring? Did everything kind of look similar to how it would look at Uber or was it a different architecture completely?"
16;Atindriyo Sanyal;00:09:32.810;00:10:46.990;00:01:14.180;"Yeah. No, I spent about half a decade there, so I kind of saw the evolution of these NLP systems going from these rules based engines to starting to use basic models like Bayesian Classifiers and Decision Trees all the way to now the more powerful transformer models that they use today. But the architecture was similar to the way you would design any end to end software stack. Siri, for example, there was a speech team, of course, which dealt with the initial sort of stream of speech data, converted that to text, which went to a separate service, which was the NLP service, which initially was a set of rules which punched out an intent at the end of it. And slowly those rules were sort of replaced with a bunch of models. And that's how machine learning kind of came into the core part of serious natural language understanding. Speech side of machine learning evolved separately. So there were always these two separate stacks of ML, one for speech and one for a natural language NLP classification which evolved separately. But they're sort of again trying to find commonalities between the two stacks and trying to centralize them under one umbrella."
17;Atindriyo Sanyal;00:10:47.110;00:11:38.270;00:00:51.160;"But overall I think the system was initially in the initial years it was designed for scale and a lot of focus was on sort of making theory faster. And it's towards the end of 2015, 2016, when they really started to make theory less rules based and more machine learning oriented. The NL system hosted a whole bunch of models which were retrained almost daily on new data. And the stack for the retraining, the automation was similar to what you would see any other company. And in more recent times though, I think with the evolution of Transformers and other kinds of models, more powerful NLP models, there has been a focus at Apple from this is of course hearsay and I hear from my old colleagues who work there still that they're building custom transformer models and their custom embeddings for Siri."
18;Simba Khadder;00:11:38.460;00:12:04.640;00:00:26.180;"It's super interesting. Yeah, we obviously released embeddinghub, so we've seen the same sort of thing of embeddingsfirst-class kind of finding their way as a first class entity of machine learning, kind of like what you talked about before. The artifacts, the embeddings kind of become their own sort of artifact. So I guess you would say probably that across NLP computer vision and the traditional tabler machine learning, the platforms look pretty similar. It's almost like you may have one or two extra add ons. Is that fair?"
19;Atindriyo Sanyal;00:12:04.780;00:13:21.720;00:01:16.940;"I think the challenges for if it's unstructured versus structured data, I think the challenges remain the same. The main sort of software engineering challenge around both data modalities is really around data management at scale and ensuring that models perform at low, latency and high throughput. So those challenges remain the same. The techniques though is where the different sort of comes. For example, if I talk about data quality in particular, there's data quality issues that you would look for when it comes to structured data models versus in unstructured data models. And that sort of really comes down to math and statistics because a lot of the unstructured data is essentially just vectors, including embeddings. So there's a lot of powerful things you can do around Embeddings using spatial geometry, even basic things like cosines distance calculations and optimizing those at scale. There are problems which are very particular to unstructured data. On the other hand, in structured data, the problem which I saw a lot at Michelangelo was data scientists essentially throwing the kitchen sink of features at a model. Over 200, 300 features would be thrown at a model and a feature store like system makes it easier to fetch features."
20;Atindriyo Sanyal;00:13:21.750;00:14:06.320;00:00:44.570;"So it kind of makes the whole process even more indisciplined in a way. So there you can have data quality tooling that can sort of measure things like feature redundancy and the relevance of features to labels. And even that has there's some element of statistics to it where you measure the distribution of the values and sort of correlate it to the labels. And that way you can eliminate in some cases we saw over 80% of the features were redundant, so you could literally eliminate it from your feature set without having any impact on the model. So there are some differences in the way you would measure certain data quality metrics for unstructured and structured data, but in principle, in the software engineering, challenges sort of remain the same. Got it?"
21;Simba Khadder;00:14:06.340;00:15:03.320;00:00:56.980;"Yeah, that's a great way to put it. I think that very much captures most of the nuance that comes into those things. I think we've done a few of these and some things that have all come up. It's funny because we're different. People live in the stack, they'll almost have different answers to these things as a perspective. So when you talk to someone who's focused on labeling, they'll say like, oh, it's completely different. And it makes sense because for labeling it is a very different problem. But for kind of feature engineering and building models, it looks similar. If you zoom out a little bit, it looks similar. Like you said, there are specialized techniques. But yeah, it's fascinating. And I think as this space evolves, we'll start to really see how the market really decides they want these ML platforms to work. Will there be one generic one that has different kind of almost like specialized deployments, or does it end up with your vision platform versus another platform? And I think it will be very interesting to see as the space plays out, for sure."
22;Atindriyo Sanyal;00:15:03.400;00:15:47.330;00:00:43.930;"I think yeah, you mentioned an interesting point about labeling which I forgot to touch upon, and I think that's one of the key differences in some of them. I think labeling is like a huge cost overhead, especially in the computer vision, unstructured data machine learning side. Most of the models, at least at Uber, we saw with structured data, the labels are kind of auto generated. So the ground truth kind of comes from the events which transpire on the system, for example, if the ride was taken or not, or if it was successful or not. So labeling is not a big issue there. But in computer vision, labeling is a very, very manual, human error prone process and it's literally a multi billion dollar industry of its own."
23;Simba Khadder;00:15:47.490;00:16:15.260;00:00:27.770;"Yeah, I could talk about this for a while, especially like what you talk aboutthe behavioral data, behavioral labels comes with its own kind of interesting gamut of problems because finding the ground truth, I used to work on recommender systems. So anyway, I'll leave it for another time. A lot there. We talked quite a bit about feature stores and data quality. I guess first I'd love to get maybe almost like your definition of a feature store. Like, what does the feature store do?"
24;Atindriyo Sanyal;00:16:15.400;00:17:36.330;00:01:20.930;"Yes, no? That's a great question. I think feature stores are a very critical part of the ML workflow and it kind of lays the foundation for any organization which sees a growing ML footprint in their company. Essentially, they are of course, by the definition of it, it's a store of ML ready data which is ready to be consumed for any model, essentially data at your fingertips for your data scientists. But I think prior to feature stores, every team would kind of have their own way of creating and managing features. Many wouldn't even have a way to manage them. It would be extremely ad hoc and would lead to a lot of duplicated work. Typically Michelangelo itself, which is Uber's ML platform, it manages over 10,000 models which are serving Uber's production traffic. And we see there's so much overlap in the set of features which thousands of models use. So the feature store sort of deduplicates the management and creation and maintenance of these features, which would in a non feature store world, there would be so much duplicated effort and engineering work which would go into creating these features. I think with feature stores, as a data scientist, you get ML ready data at your fingertips without worrying about the complexities of how these feature values are kept up to date."
25;Atindriyo Sanyal;00:17:36.440;00:18:00.020;00:00:23.580;"And along with that I think a critical part of the feature store is also giving the right APIs to be able to do these custom transformations and aggregations on the feature values. A lot of feature values are essentially aggregates. So an API which provides these aggregations out of the box is a good sort of feature store API. So I think that's what a feature store is in a nutshell."
26;Simba Khadder;00:18:00.170;00:18:26.410;00:00:26.240;"Yeah, one thing I want to make you zoom in on because obviously I talk to a lot of people at feature stores and one piece of confusion I see a lot I'm curious for your take is some people define a feature store very literally, which is like it's where features are stored, where others would define it as where features are kind of created. It's almost like defining a feature as the row of data in the database or the feature as the logical transformation. How do you think about that part of feature stores?"
27;Atindriyo Sanyal;00:18:26.530;00:19:36.440;00:01:09.910;"Yes, not totally. I think that in my opinion, I think the real sort of definition of a feature store is a combination of the two because in the end the actual value of the feature. Of course it's really any data which can be stored in any data store. But yes, there is a logical sort of component to it which really gives the definition or separates a feature from any data point which is stored in a database. I sort of call it the metadata layer of feature stores which really stores the actual definition of the features. As I said before, features can be all different types. They can be numeric, they can be categorical, they can be aggregate, they can come from different sources, they can be real time, near real time, they can be historic, they can be embedded. There's a lot of unstructured features as well which go into models. So all this definition is sort of captured in the logical layer. And that layer itself is a gold mine of metadata and information which you can leverage and provide a lot of powerful search capabilities, recommendation capabilities. It's literally your model universe, this metadata."
28;Atindriyo Sanyal;00:19:36.590;00:20:33.440;00:00:56.850;"There were some efforts at Michelangelo which I was leading around feature search and discovery where you could see which features are used in which models and you can literally do machine learning on top of that to recommend the right features given your models and your use case. So a feature store encapsulates both this logical layer which to me is sort of the application layer of the feature store and it can give you a lot of very powerful capabilities. But then there's the storage, which is also equally critical because features can scale out of proportion, it really depends on the scale of your organization. But there's a lot of infrastructure optimizations you need to do to be able to manage features on a day to day basis, ensuring you don't blow out storage and your compute is not going out of whack. So to answer your question, I think the real definition of feature store is a combination of both."
29;Simba Khadder;00:20:33.830;00:21:18.470;00:00:44.640;"Got it. And I guess now that the feature store. I remember when we built our first feature store in my last company, we didn't call it the feature store. Everyone has a feature store. Like, if you have features, like they're somewhere there's some sort of feature store. But we didn't call it that, and we didn't really make it something that we focused on at first. Even though the problem space has existed for a while, has been solved for a while. It's new that people are this whole space exists, and there's actual teams built around this type of new infrastructure called a feature store. And now there's obviously a lot more examples. In the past, there wasn't really any examples for us to look at. We're kind of making it up as we went, really focused on, like, more streaming problems, less than feature problems."
30;Atindriyo Sanyal;00:21:18.630;00:21:19.170;00:00:00.540;"Right."
31;Simba Khadder;00:21:19.280;00:21:31.260;00:00:11.980;"Well, all the context you have now, what do you think that MichaelAngelo's feature short did well? What do you think it's something that even with all the new stuff that's come out, you think that Michelangelo was uniquely to that and how it was designed?"
32;Atindriyo Sanyal;00:21:31.410;00:22:51.840;00:01:20.430;"Yeah, I think the one big thing that Michelangelo solved really well was being able to deploy a model and serve it at very high scale, at very low latency. That was the first big problem which Michelangelo solved really well, and that kind of became the primary reason for data scientists to come to the platform in the first place. So when the whole system started you're right that the whole concept of feature stores per se was evangelized eventually later. But the problem is, it's a software engineering problem, so it's kind of existed for a while. So before even the word feature stores were formalized at Uber, the infrastructure still existed. And part of that infrastructure solved this problem of deploying models at low latency really well, to the point where data scientists would bring their custom trained models trained outside the Michelangelo platform, and they would just hand it over to the team and just say, hey, just deploy this for me. And I don't want to solve the scale problem. It's too hard, and I just want to focus on my modeling and I want to solve this use case. But I trust the platform because it's able to achieve P 99 latencies of single digit milliseconds at extremely high throughputs."
33;Atindriyo Sanyal;00:22:51.930;00:23:19.190;00:00:27.260;"So that was one big problem which was solved very well at Michelangelo. And eventually, of course, the platform itself evolved to automatically train the model and automatically deployed to production. So it sort of started with this service which was able to manage and serve models at scale to eventually sort of spanning out and becoming an end to end ML platform. But that was one thing that Michael Angelo sold really well at the outset."
34;Simba Khadder;00:23:19.320;00:24:11.220;00:00:51.900;"Yeah, it's super interesting because like now when I go into companies, I see there are two types of problems. There's kind of a processing problem and the organization problem. And it feels like most people who build feature stores, they hit the processing problem because then it's like, that's an impasse. You have to solve the, like, hey, we need this thing in production and needs to have this latency, we need to have a way to solve it. Where the organizational problems of management versioning, all that stuff, we see people kind of just try to like duct tape around and just kind of sort of make it up as they go. And those tend to be kind of the other set of problems that feature stores solve. And yeah, it's interesting that the processing one, I think is a really crucial one. And it makes a lot of sense that Uber would be so good at solving that just because there's streaming first almost from the get go is what it does. What do you think you would have Michael Angelo's features tried to do differently? In retrospect?"
35;Atindriyo Sanyal;00:24:11.370;00:25:20.730;00:01:09.360;"Yeah. I think in hindsight, one thing we should have done differently or I would have done differently was sort of thinking about the importance of data quality. And it might come across as I am, me being biased because I'm working on data quality now. But it truly showed this problem was highlighted quite a bit when I sort of spent the last year or so of my stint at Uber leading data quality for machine learning efforts there. And from a lot of the tooling that we had built and sprinkled into the Michelangelo platform which asserted data quality, it showed immediate improvements in the performance and stability of thousands of models, which is powering Uber's applications. So a lot of the learnings that came out of that, I've sort of taken and decided to build Galileo. And it's really sort of give the power to data scientists to be able to ensure that they're training and evaluating their models on the highest quality data and giving data scientists the tools to quickly inspect and analyze and fix the data so that they can punch out a high quality machine learning model in very less time."
36;Simba Khadder;00:25:20.900;00:26:04.290;00:00:43.390;"Got it. I think a lot of MLOps use cases or problem sets that we saw most people blowing startups. It's a problem you see when you go solve it. And it's almost best when you that's a problem that you yourself have either had to solve or face. I know that's true of us and it sounds like it's true of you all, I guess. So data quality makes sense. You want your data to be high quality, I think. I don't think anyone's going to argue about that. But I guess I'm trying to visualize what you mean about data quality tools. Like how does it look? If I'm a data scientist, I might be doing some experimentation. I might notice that there are gaps in my data. I guess I'm trying to see how it's almost like a new category, and there's a lot of categories being created and kind of merged at all times. So just trying to understand how you define this category, what does it do?"
37;Atindriyo Sanyal;00:26:04.460;00:27:20.080;00:01:15.620;"Yeah, no, I agree that the term data quality is so broad it can encapsulate any and everything on Earth. I think one aspect of it is sort of bringing the discipline of a unit and integration testing to models in a way where you can literally have a unit test, for example, that asserts that the number of misclassifications in your test set should not be greater than ten. And if that is true, then you fail the pipeline. That's the simplest sort of data quality test that you can write. So that's one aspect. There are other aspects to data quality as well. Say, before you even train your model, you have holds of data in your warehouse and the first question you ask is, what data should I train my model on? So choosing the right data, and that also includes the quality of the data in the sense that there's less null values or there's less garbage in the data, but also choosing a minimal set of the data to achieve the same model performance. For example, you don't need to train your model on 100,000 data points when you can achieve literally the same amount of evaluation or test performance with 30 data points."
38;Atindriyo Sanyal;00:27:20.220;00:28:37.720;00:01:17.500;"But you have to be very scientific in thinking about what data you want to train. So this is another aspect of data quality. I think the third aspect of data quality is once you train your model, your data set fits differently with different model architectures. And the most typically practice, which is followed by data scientists, is to tune hyper parameters, change the model architecture, introduce new hidden layers, or changing the depth of your tree, and then retraining the model and seeing how the data fit. And this whole process goes on and on. But one of the realizations more recently has been that you really need to shine a light on the data to be able to see certain regions of your data set where the model is underperforming and giving you the tools to be able to automatically sort of figure this thing out instead of this being like a manual process. In a lot of companies we've seen, including very tech first companies, data quality assertion post training is almost like this detective work, especially for unstructured data, where you literally take your data set and you put it in an Excel sheet and you try to figure out patterns, look at model confidence scores, and you try to see where the model did well, where it didn't do well."
39;Atindriyo Sanyal;00:28:37.860;00:29:48.920;00:01:11.060;"And then you move to your next iteration of training, either by removing a certain set of your features or adding new data to your data set. And this process goes on and on and it takes months because the whole process of inspection is very manual. So providing tools to be able to automatically infer regions of model under performance or regions where the model performed well. So you don't need to worry about those regions. So that's the third aspect of data quality. And the fourth and final aspect of data quality is, of course, once you deploy your model into production and the data changes over time, it drifts and the model starts performing differently. So really shining a light on monitoring not only the predictions, but also feature drift and correlating feature drift with feature importance and having proper alerting and monitoring around that. So those are sort of the four big pillars of data quality which I have seen in my experience. But as you said, it's sort of this very new evolving field and it's always very exciting to see people discover new aspects of data quality and learn from others who are in the community."
40;Simba Khadder;00:29:49.310;00:30:48.410;00:00:59.100;"Yeah, it's a really interesting way to break it down. And the first and fourth pillar have become a little more accepted in the loft space. The fourth pillar being monitoring feature drift. A lot of people talk about feature drift, the first one being kind of the unit testing of data pipelines in your data where things like gratification, this is not a new concept, but obviously it's part of a bigger picture. I think the second and third pieces are very interesting. And even though we did it ourselves, we were doing recommender systems again. So it was very important for us to figure out which subset of our users were we not doing well with. Because recommended systems is a great problem. You might have something that does better as a whole, but it gives certain subset of users such a bad experience and that's almost worse than doing a little worse and giving everyone a consistent experience. So it's interesting to talk about. It's almost like the data set optimization, it's almost like being able to it's less the cleaning and less the monitoring. But 2nd, 3rd pillar is much more about picking the right features, picking the right roads to train on handling."
41;Simba Khadder;00:30:48.480;00:31:04.280;00:00:15.800;"Even like with drift. Like one way I've seen people solve drift is just cut lots of old data. Like after the pandemic hit, I saw this trick where a lot of people were just like over sampling host pandemic data to try to kind of fix the weird distributions they were seeing."
42;Atindriyo Sanyal;00:31:04.610;00:31:16.650;00:00:12.040;"No, totally. I think you've kind of nailed it. I mean, of course you are also very involved in this space, so it's good to get your experience as well. But overall, I think you pretty much nailed it."
43;Simba Khadder;00:31:16.820;00:31:32.780;00:00:15.960;"Okay, so I'm looking at the sale quality platform. There's a million people, different parts of the ML pipeline. I can't do everything at once. How should I be thinking about this? Why should I be choosing to bring this into kind of my initial or midway done MLOps platform?"
44;Atindriyo Sanyal;00:31:33.170;00:32:51.950;00:01:18.780;"Yeah, absolutely. I think the answer is simple as well as complicated. I think the simple answer is that it's a no brainer that the data is the lifeblood of your models. It's garbage in, garbage out. So if there's no disciplined way of thinking about what data you're training on, it is almost a certainty that at some point your deployed model will perform badly and it won't give you the results which you desire. And the increasing adoption of machine learning and businesscritical use cases, a bad model can have catastrophic business outcomes. So therefore, the discipline of data quality, baking in the discipline of data quality, I think is super critical to ensuring that you don't perform embarrassing mistakes in your production environment. There have been many cases, time and again, where a lot of big enterprise, they see models as this voodoo black box, or even machine learning for that matter, as a black box which they don't want to adopt because there's higher risk in deploying a model for critical business use cases. But from my experience in building these systems for many years, I have seen that almost always the issue is with the data."
45;Atindriyo Sanyal;00:32:52.110;00:33:06.090;00:00:13.980;"And tooling like data quality can always ensure that your models are robust, they're compliant, they're fair, they're unbiased. And that's the only way you can sort of deploy your models with confidence. Cool."
46;Simba Khadder;00:33:06.140;00:33:34.890;00:00:28.750;"So I guess one thing I'm trying to understand is and I think I got it, but just to make it super clear, when people think about data quality, when I do, at least the things that come to mind are things like Great Expectations. Like a certain base, kind of almost like blocks in your pipeline to make sure that data is coming in and being fitting your expectations. And it sounds like what you do is much more than that. And I guess I want to understand what's the difference between how you're thinking about data quality in Galileo and something like Great Expectations?"
47;Atindriyo Sanyal;00:33:35.450;00:34:39.409;00:01:03.959;"That's a great question. So Great Expectations is a really good tool for ensuring there is a robustness in the data set itself by it's almost like unit testing your data set, where you can programmatically have assertions and expectations of the range of values you expect in a column, for example. But I think with ML data quality, it's a lot more than that. I think with Galileo, for example, we are trying to gauge into how well a model would perform on, say, older patients who are female who live in California. So that's a more higher level of an abstraction. And these kind of questions can only be answered once you train your model with a data set. And it's really about shining the light on how the model performed the data, as opposed to testing a data set on its own. So I think that is one of the fundamental differences between a system like Galileo and a system like Great Expectations."
48;Simba Khadder;00:34:39.600;00:34:59.740;00:00:20.140;"Got it. So it's almost like you talked about the experimentation thing that's the on top and you kind of fit more into you tie into that a bit. You're kind of at the higher level of traction your mark. Great Expectations is almost something you could argue is just like a data ops tool where what you're doing is very much clearly MLOps. It's oriented their own models and machine learning."
49;Atindriyo Sanyal;00:34:59.940;00:35:44.080;00:00:44.140;"Absolutely. ML data quality is almost a separate discipline of data quality. It's a subset of the larger data quality system. And I think both are extremely important. Just in general, there's a lot of low hanging fruit which you can sort of get from a general data set by applying a system like Great Expectations. But beyond that, once you train your model on that data set, there's a lot of insights which can come out of the way. The model will fit the data, and you can leverage these metrics which are flowing through the guts of the model to be able to infer a lot more very interesting, nuanced, model centric insights. And that's a big part of machine learning. Data quality."
50;Simba Khadder;00:35:44.280;00:36:08.960;00:00:24.680;"That's awesome. That's an awesome way to kind of think about it and break down the two categories or subcategories of this kind of very large, overlying data quality umbrella. There are so many things that we've covered and so many more things I wish we could talk about. I know we've kind of been going for a little over 45 minutes now, so thank you so much for off and on. It's always great being able to chat with you and get your insights on the space as it's growing and yeah, thanks for answering all my questions."
51;Atindriyo Sanyal;00:36:09.290;00:36:11.760;00:00:02.470;"Thanks so much for having me. It was a pleasure talking to you."
