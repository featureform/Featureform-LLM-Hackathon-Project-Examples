Number;Speaker;Start time;End time;Duration;Text
0;Speaker 1;00:00:06.070;00:00:27.570;00:00:21.500;"Hey, I'm Simba Kader, and you're listening to the Amelop's weekly Podcast. Today I'm chatting with Stefan Kravchik, former manager of data platform at Stitch Fix. He built and later open source the Hamilton Framework while he was there. And prior to that, he was an ML at LinkedIn next door and an MLP for enterprise startup. In his own words, crashed and burned. Stefan, so great to have you on the show today."
1;Speaker 2;00:00:27.680;00:00:30.980;00:00:03.300;"Yeah, thanks for having me. So excited. I gave a quick outline of how."
2;Speaker 1;00:00:31.000;00:00:34.600;00:00:03.600;"You got down Lost. I'd love to hear it in your own words. What was your story that got you."
3;Speaker 2;00:00:34.620;00:01:36.390;00:01:01.770;"Into how did I get into MLS? Well, I originally did computer science at Stanford and AI specialization. So I knew I wanted to do something around this new field of ML AI. So I guess at LinkedIn I kind of pivoted to kind of prototyping content based recommendation products. I got first hand experience like what it's like to build a bottle and try to get out to production. And next door I built similar things and related tools and technologies to first version of Data Warehouse that then we could build and get data to train models with and online experimentation and testing frameworks. That's what led me to go to the startup because I wanted to get better machine learning frameworks because I really wanted to understand how to operate them, how to build them, because I thought that was the future. So then the opportunity came up as Districts was like, come and help build a machine learning platform. Because that's kind of what before Mlaps was coined. That was, I think, people were trying to build machine learning platforms. And so that's what I was kind of interested in doing. And then, in which case, while I was at Citrix, I was a little more than just a platform, but just trying to help data scientists get things to production themselves."
4;Speaker 2;00:01:36.560;00:01:48.100;00:00:11.540;"The MLPs term was coined, so it was great. This is the easiest way that I can explain to someone what me and my team does that we're trying to help data scientists operationalize machine learning in a way that's self service. I love that."
5;Speaker 1;00:01:48.120;00:02:17.020;00:00:28.900;"Yeah, a lot of these terms we fill up with feature stores where I'll define a feature store to someone and we're like, oh yeah, we have that. And it's like, yeah, I mean, everyone built all these things. Anyone who's been in machine learning production has been doing Amal Ops forever. We didn't call it that and we didn't really even think about it that way. We didn't really break it down in that way. It was just like the platform for machine learning. For us, the feature store was something we called it our data platform for shitlining. Like, it wasn't the feature store that."
6;Speaker 2;00:02:17.100;00:02:18.760;00:00:01.660;"Turned kind of came later and we."
7;Speaker 1;00:02:18.780;00:02:20.540;00:00:01.760;"Said, yeah, that's what we built. It's a feature store."
8;Speaker 2;00:02:20.680;00:02:52.970;00:00:32.290;"Yes. I think it's like it's somewhat how DevOps came about. It's just rather than handing off, you're now trying to do things yourself and a best practice way to get things to production. In which case I was like, yeah, I mean, adding a feature store and thinking about features and models and data, it's a whole ecosystem. In which case I think, yeah, it makes sense. I was happy that whoever came up with it, came up with it because I'm like, great, this makes my job much easier to explain. But I was thinking about software engineering best practices and again, trying to bring in the DevOps mindset of like, how do you operationalized things without breaking production? You want to stop bad things from happening. So what's a good way to think about and do it?"
9;Speaker 1;00:02:53.070;00:03:00.050;00:00:06.980;"You kind of answered this question, but I want to just ask you. So it's, I guess, discreet for anyone listening. How would you define ML?"
10;Speaker 2;00:03:00.180;00:03:31.760;00:00:31.580;"Yeah, riffing on what I just said before. For me, the whole goal of The Mindset of Mops is to stop bad things from happening in production, but you're doing it and approaching it in such a way that you're bringing, I guess, what's termed developer operations or DevOps kind of best practice for deploying and getting to production. But then you're also thinking about software engineering best practices because machine learning is also data and code that you ship and package together. So it's really this kind of mindset of how do you stop bad things from happening in production? And then what are the techniques and ways and framings that you can kind of employ to make that happen?"
11;Speaker 1;00:03:31.840;00:03:53.330;00:00:21.490;"Yeah, I love that you brought up DevOps. It's interesting to see how everyone view of MLPs versus DevOps. They're always two different things, but some people say they're completely different. Other people are like, yeah, it's the same thing applies to different places. Some people I've talked to, some people are like, yeah, it's like 90% the same. MLS do not have to exist. How would you define the difference between MLPs and DevOps?"
12;Speaker 2;00:03:53.400;00:04:38.180;00:00:44.780;"For me, DevOps, when I encountered it was like, oh, rather than it being someone else's job function, a job role, rather to deploy my code, I now, as a developer, deploying it myself. Like, for me, I think, like, MLOps. If you think about if you take DevOps next, how do you enable someone to develop and deploy and develop things as part of their job function? Then I think to me, that's similar to an MLS mindset now. DevOps, I think, is I want to say it's probably right now. To me, it's actually a subset of ML apps, right? Because if you think about deploying to production, then maybe there's a little bit of disjointness and DevOps because it focuses on things that deployments that would never be machine learning deployments. But I think right now, one DevOps is a subset of the other and helps feed into, like, any best practices and DevOps, I'm pretty sure you can bring them back into ML Ops. I love that."
13;Speaker 1;00:04:38.200;00:05:30.580;00:00:52.380;"It's funny, in the last episode we had, we had James who is an investor and he said one open question is the size of the DeVos market versus demos market. You're not exactly saying that. But I love the idea of like well, that lost market, super putting models of production. It kind of hits every pain point of data offs, DevOps and just all these other things in between. So you're kind of right. I can't imagine someone having a really good MLS workflow without also having an amazing devil's workflow. Same with data ops. It's almost like you need both for the foundation and then you need to find a way to apply that for your machine learning team to kind of get it all working together as ML Ops. Could you start some things that I guess manifested in what makes so putting a service in production versus putting a model in production? Could you talk about we dissolve differences if you talk about maybe a story of something that happened that could only happen in machine learning, but you have."
14;Speaker 2;00:05:30.600;00:06:34.400;00:01:03.800;"To solve yeah, I think it's all right. What's the model, the function with think about it from the principles in terms of like, hey, you want to predict something? Well, there's going to be some function somewhere that you pass some data in and then that function actually has some state, has some internal state that is like the coefficients of a model. And so together with the inputs, you're then transforming and using computing something with the internal state and outputting a result. And so if in traditional, I guess software engineering apps or services, there isn't like really any internal state, it's all pretty invariant and very easy to reason about about the inputs that you get in the output. So you can write unit tests and as long as you have great unit test coverage, pretty much there's nothing wrong. It can go wrong production. But with Mops and building machine learning model into production, I think you have to really think about all the different there are so many more I guess you could say things that could go wrong. I wouldn't say if you have a model rather time so there are so many components to putting a model into production that can go wrong really."
15;Speaker 2;00:06:34.480;00:07:36.950;00:01:02.470;"Whereas with the traditional app, the thing is invariant given time. So as time goes on, as long as expectations that you coded things, the inputs that you expect come in as expected, you don't have to update or change that with a model. That's not necessarily true, particularly since your model if your inputs shift and kind of inputs values over time, then maybe the internal state that you have in the model actually becomes to be less relevant and actually starts output and change different results. Whereas in the other app everything is pretty much hard coded and it's very easy to kind of reason about with the model that's not quite the case then it's well it usually happens that you're also updating models probably far more often than your power updating the internal app logic. So if you think of that function endpoint and how many different versions there are over the course of over time as you update the model you have to get the update to the app somehow. So how do you do that in a way that doesn't break things once it's in there? How do you reason about ensure that things aren't changing for the worse or if they are changing for the better?"
16;Speaker 2;00:07:36.990;00:08:16.670;00:00:39.680;"Like you need a bit more measurement and kind of observability around it. What happens in production then also flows back to also creating a better model. So you kind of have a loop or at least you should always be thinking about when you're deploying a model that this is going to be kind of the feedback loop that you have to kind of think about whereas with the traditional app I think you can be one and done it and kind of other than adding new features to the app which is where DevOps practice I think really help from that sort of perspective but in terms of once you've kind of created a feature it's probably not going to change very much unless the business changes and that changes are on a much slower pace than on the pace of our model as it kind of evolves."
17;Speaker 1;00:08:16.740;00:09:16.730;00:00:59.990;"Yeah, I like thinking about because for us we were dealing with a lot of companies but for our recommender system I'm sure you don't but quite a bit of stitch fix and yes, one of the problems that we always ran into is there's no such thing as like the perfect recommender system and sometimes, for example, a user would say this user got a really bad recommendation. It's like well how do you fix that? I mean one option is that you write some if statement band aid on it or other options you try to retrain the model and fix that but then it becomes kind of whack them all and you can't unit test it right? Because it's kind of what the model has learned and so we have all kinds of like well I mean there's a lot of stuff we built to make it possible and these are things that would never happen like you said and something that is much more a standard programming like if something is wrong you can go and debug it. If a model is outputting garbage, sometimes what you do, it's not really a good answer. You can retrain, you can just try things and hope it fixes it or you really have to there's no proper."
18;Speaker 2;00:09:16.770;00:09:17.200;00:00:00.430;"Way to do it."
19;Speaker 1;00:09:17.220;00:09:26.750;00:00:09.530;"You can't just go in and say hey, I'm going to change this weight to 1.7 and then it will all work out. And I think that's kind of where a lot of these problems come into play."
20;Speaker 2;00:09:26.850;00:09:50.540;00:00:23.690;"Yeah. I think the way that I kind of think about it is a model statistical, right? So there is rather than it being a binary outcome, which traditional programming you kind of do with a model, there's a statistical set of outcomes, in which case there is this kind of well, there are some areas where the results aren't going to be that good and we're not going to know what to do here. Right. So definitely bring some uncertainty into running a production service."
21;Speaker 1;00:09:50.620;00:10:11.910;00:00:21.290;"Yeah. One thing you talked about is we already talked about a bit, but I want to bring it back to the forefront is in the early days of MLOps, we didn't even call MLOps, and nowadays there's a whole set of startups, there's a whole space, there's all these categories that we've defined, I guess. How did them lobs change over the years, from your perspective?"
22;Speaker 2;00:10:12.080;00:11:25.900;00:01:13.820;"Yeah, good question. I think back in the day, people were such as myself, like we were an embedded engineer and a team, and we basically did things end to end. And then as companies start bringing ML back, then it was like, well, you need a little bit of platform to centralize some of this cost. And so I think that's where to move to kind of platform and building email platforms. So Uber Michelangelo is a classic example of this. And then I think people realize that, well, there's a whole ecosystem around it. Do you need to build a fully blown solution like Michelangelo to get any value? So I think it's like the mindset of practice, therefore, I think right. How has it changed? I think there's definitely more widespread adoption of machine learning and people realizing that they have similar problems. But then also I think realizing that there isn't one solution that fits every single kind of use case, ad serving technology and machine learning models there is very different than predicting health outcomes. They have different, you'd say, velocities of model retraining and stuff like that. And so I think there's been a more emergency and kind of acknowledgement, I think from industry that there are all these different problems and not you can't build a single platform that will serve everyone."
23;Speaker 2;00:11:26.040;00:12:11.210;00:00:45.170;"People have tried. I think they haven't been going too well. But yeah, I think there's also the people who are doing Mlaps has changed over the years. So I think it was very much CS heavy computer science kind of background, basically. Now I think it's anyone who comes from the other end who's coming from, hey, I know how to build a model. Say physicist is a statistician. How can I now think about deploying and pushing things to production? And in which case I think potentially like less software engineering, classical training background, people probably getting into things versus before. Basically, if you were a machine learning engineer, you're almost guaranteed to have a software engineering background now. I don't think that's the case, which impacts, I think the framing and who's using and the pitch and tooling for M apps."
24;Speaker 1;00:12:11.280;00:13:07.210;00:00:55.930;"Yeah, I love that breakdown. There's almost 30 stages that you mentioned. Stage one was MLS wasn't a thing. You were just an engineer on a machine learning team, or a team that happens to be doing machine learning and your boat was just getting production. Stage two was like, hey, we'll build a platform team that kind of does this for all company. And then stage three is kind of where we're at now or what we're getting into, which is, hey, we don't really need to have a custom platform per company. Like a lot of these things look the same in different places. There's lots of nuance. So we really need is almost like a set of tools, like DevOps tools that we can bring in and configure to fit our workflow. And it is interesting to compare DevOps to this because it's very similar. Like stage one was you have an engineer on the team and their goal was just to get it into production somehow we'll build it like that person's job or maybe it would share them up, engineer it's like get in fraud, make sure it doesn't go down. If it goes down, like, you get a call at 02:00 A.m. And you go figure it out."
25;Speaker 1;00:13:07.330;00:13:37.450;00:00:30.120;"And stage two, which will build our own DevOps platform like Google built.org, and there's all kinds of other examples of DevOps tool sets that were created. And then I think now we're kind of in that third stage, which is, hey, we don't need to go and build Borg Kubernetes exists. Even like all the hash corporate tools exist. Like CI CD exists. There's all this other open source tooling that we can take up and bring together workflow. It's kind of a bit of like the same thing happening over again, but just applied in a different for a different problem space."
26;Speaker 2;00:13:37.630;00:13:44.180;00:00:06.550;"Yeah, totally. I mean, I think it's like something like that says something about the maturity of how solutions going to permeate through industry, I think."
27;Speaker 1;00:13:44.260;00:13:52.290;00:00:08.030;"Well, with all the changes that are happening, how do you yourself keep up with everything? It feels like there's a new blog post, new something every single day."
28;Speaker 2;00:13:52.400;00:14:50.590;00:00:58.190;"Yeah, I think it can be overwhelming, but I think my go to are generally Twitter, the way I discover things. Twitter is one way. I think there's engineering blocks on the tech companies and then there's like the rise of substance and all the various newsletters. I subscribe to a bunch of them that kind of thought were kind of interesting. I end up what I end up generally doing is skimming headlines and then bookmarking things later that I want to kind of read. But I don't think I have the best methodology for the things that get the most bars. Generally I probably haven't seen because of the way I look at things, but otherwise it's useful from the things that I have bookmarked as trying to get an understanding for the mindset of like, why was this thing created? Because I think to the point where there isn't one solution that fits all. People create solutions for different reasons. And so I think it's useful to if I can actually live through those kind of posts and things, actually, I'm trying to understand why it was created in the first place. Like was it an organizational thing that led to this particular solution versus them building another one?"
29;Speaker 2;00:14:50.710;00:15:21.870;00:00:31.160;"That's an interesting kind of question to ask sometimes with these decisions, because it's not true with Mlaps that every company has the same topology, in which case, like, different solutions you might be selling to different people even should I build versus buy? It really depends on your company topology. So at least from staying up to date, I try to step back a bit, at least ask the question, why was it created and what was the environment that created it? So maybe there's something to learn, like, oh, okay, smaller companies do this or big companies, they have these problems, right? And so that's the kind of at least why that things I love to take away from reading posts and things."
30;Speaker 1;00:15:21.980;00:15:31.640;00:00:09.660;"Can you name drop a few people that people should follow or some substantials or anything, just to name a couple for people walk listening who want to at least get started."
31;Speaker 2;00:15:32.390;00:16:37.090;00:01:04.700;"So your mileage may vary. So I'm terrible at remembering names. I just know that it comes in my inbox. The last one that I remember reading was the sequence on substance. Otherwise. On Twitter I follow Sarah from Amplify Partners. So tweets and data ecosystem. Email. Ecosystem. There. I think if you know of any of the open source projects, follow their Twitter handles, maybe follow some of the core contributors on them. You'll kind of get the updates. Maybe they'll go to a conference or something and then you can pick things up that way. But I think I have Josh Wills from the data engineering side, I followed the ML kind of AI gurus. Andrewing Andre capacity, I think, try to help people from the various big tech companies follow. There is Sebastian Ramirez from FastAPI. I think since I've been living in a Python based world, also focus on Python based things. And so following a bunch of those people there, try to follow some people from data side. Am I upset? You can follow me or you simply and then from the actual machine learning infrastructure and where industry and interesting things that are happening, it's follow the AI ML research people."
32;Speaker 1;00:16:37.150;00:17:08.510;00:00:31.360;"Yeah, everyone should probably follow a mix of each section. Like you need to stone tapping AI specifically from practitioners to vendors, you kind of need to, I guess, cast a wide net to really kind of make sure you catch the things that matter. There's a lot of noise and there's just a lot of things being figured out. It's almost like trying to keep up on papers. Like you just can't you just have to kind of find a way to figure out what is and isn't worth keeping up with. And you have a cloud coming out on August 22. Can you share some more about it?"
33;Speaker 2;00:17:08.609;00:18:17.940;00:01:09.330;"Yeah. So I'm partnering with Sphere. So they're a recent YC batch company, and it's called Mastering Model Deployment and Inference. The idea is it's kind of like an executive ed style class for practitioners. So those four to our kind of sessions, the goal is to try to give people the skills and ability to know how to improve latency and throughput by thinking about how do you select appropriate inference architectures, how do you reduce outages or the meantime to resolution, how do you do that? What are some common model observability approaches to do so? What's the overall kind of macro architecture and what is the impact of your machine learning architecture with respect to reliability, scalability, and getting modes to production? If you're thinking about questions such as what components should my model deployment system have, where will my current approach to deployment and Frances breakdown? The class isn't going to be all lecture based. It's going to be me lecturing a little bit, but then it's going to be some group work or like you can say, group discussion. Hopefully there'll be some other machine learning engineers or people who deploy stuff to production. There'll be some interesting networking, there'll also be some interesting questions asked of this problem, etc."
34;Speaker 2;00:18:17.970;00:19:15.250;00:00:57.280;"So you can maybe learn from your classmates. So hopefully by the end of the class, learners, you'll be able to answer questions that something like the following. So what are the components that my model deployment system should have? I think it's kind of an interesting one. Not everyone needs every single component because I don't think it really depends on your SLA and what's the cost of an outage discussion as to what components should you have and then maybe even helping you take a critical look at what is my current approach to deployment and inference and where is it going to break down. What is going to be painful for the business or for me. And then what do I want to do about it? What are some architectural approaches or changes that I could attends that I could use to help make a decision? What are some architectures such patterns or tools that can help me reduce my outages or my mean time to resolution? So a class that I guess we'll try to pack in a lot in four sessions, but the idea is that it's framework agnostic and that you can kind of try to take away some general mindset, thinking, and patterns that you can apply to your particular context."
35;Speaker 1;00:19:15.310;00:19:30.320;00:00:15.010;"Yeah, I love that. Yes, we'll have a link at the bottom. So if you want to check it out, if anyone checked out, you'll be able to learn more there. Well, let's talk about SIsix as your last role. You built out a lot of the ML infrastructure there. Maybe you could share. How did the ML workflow look like at Stitch fix?"
36;Speaker 2;00:19:30.400;00:20:27.500;00:00:57.100;"Yes, for those who don't know, Stitch Fix has 100 plus scientists which kind of role was to Iterate prototype productionized and then beyond call for models. So we're trying to build tooling and abstractions to enable them to software engineering Lego bricks, pull things off the shelf and create their workflow and getting things to production with our software engineering Lego bricks rather than them having to engineer things themselves. So that's the kind of the mindset and kind of the thing that we're going for. We were always competing with people doing things themselves. And so which case the task of my team was to build better tooling and get people to adopt our stuff. So by the end of it, we were working on a way to kind of a YAML config and Python code driven way to kind of creating machine learning pipelines. So the idea was to how do you enable people to kind of more easily manage and create pipelines? Because that was kind of a bit of a problem it's district where everyone kind of wrote their machine learning pipelines in different ways. How do you standardize it? So we'll make headway. So the idea was."
37;Speaker 2;00:20:27.580;00:21:18.750;00:00:51.170;"Well. If we can get people to kind of write these kind of configs that then would under the hood essentially compile down to kind of air flow jobs. How do we kind of build this API layer that we're not leaking airflow. We're not leaking too much of the context. But we're standardizing and trying to simplify how people specify things in a way that helps ensure that things aren't too coupled so that if they want to reuse something or change something. It's not going to be a lot of pain and effort to do so. And that was then all built on top of a little kind of framework we called the model envelope. So it's like our ML flow model DB type, analogous kind of technology. So the model envelope metaphor with the model envelope as the metaphor kind of I'm trying to suggest is you have a model in an envelope and then you're shoving things not only about the model, but things about it into it. And then we're packaging out in an envelope so that we could then use it in various contexts without you having to write any code to do so."
38;Speaker 2;00:21:18.800;00:21:59.490;00:00:40.690;"At Citrix you could write a model, save it, and then you could have it deployed into production and under an hour because we could order to generate the kind of the web service code for you. And so getting to production was at least a model to production was pretty easy. Getting features to production, it was a little harder to kind of implement things potentially two places, or we're trying to fix that, move on that, to try to make it only one. And then we're also trying to simplify the management of model pipelines. Over time, as the team grows, you generally inherit and have more machine learning models or more pipelines that grow. So how do you kind of manage that? So that's where that effort was going to help teams not have to incur tip deck by relying on a platform less so their own code."
39;Speaker 1;00:21:59.540;00:22:15.590;00:00:16.050;"So a lot of it was kind of standardizing once you have the weights or whatever, like the model itself standardizing everything around it, how is it deployed, what it needs, inputs, all that, and making it so that once you fill all these configuration files for me, I'll make it so in production."
40;Speaker 2;00:22:15.660;00:23:22.610;00:01:06.950;"Yeah. So the model envelope was, you could say, independent from the system for kind of defining your model training pipelines. But essentially with the DevOps and ML opts kind of mindset, we're trying to put in the hooks and things so that people didn't have to make different decisions on how do you log things in a web service. We just kind of standardized it by actually saying, well, we'll just generate the web service for you and it's like one of the things that we should save about the model. So hence we introspective the parcel environment to make sure that we capture the passing dependencies exactly as what they were so that we can one always reproduce the model, but then to have a pretty good idea of what's required to run it in production. And then with the kind of system to kind of simplify model training and model training pipelines, it was around a lot of people wrote model training code that was highly coupled to their context and it was very hard to share that. And so how do you do that? While you need a couple of how you provide inputs into the model training process, in which case it was like a software abstraction with config to kind of split the two apart of like how you create and features data and provides training and make the training the process of creating a model pretty standardized in a way that is agnostic of how data gets to it."
41;Speaker 1;00:23:22.650;00:23:32.780;00:00:10.130;"What's the design decision that you make in building this that perhaps someone listening to building like a mobile survey or similar thing? What's something you decided that you can maybe share? That worked really well."
42;Speaker 2;00:23:32.860;00:24:31.700;00:00:58.840;"Yes. I think the easiest part is to talk about, I guess, the model envelope. So one of the ideas was we were trying to we don't have an API that someone triggers to deploy a model at the end of the pipeline. So if you look at a bunch of these frameworks they have you saved the model, but then. Somewhere along lines is deploy model step. We explicitly made the decision to kind of not have that, and instead people then need to go to our little web service kind of UI and write a little kind of a rule based, little set of rules that would trigger a deployment. And so that was very easy for us to then insert and stick in a CI CD step so that we could enforce one. So for anyone who wanted to deploy something, we could make it very easy for someone to like, oh, you want to deploy this model? Okay, well, maybe you want to deploy staging first, do some things with staging, say that it's good, and then we'll kick off a production deployment. Whereas if we had allowed people to kind of stick and deploy model at the end of the script, we wouldn't have had as much control with respect to inserting and upgrading and changing how moderns are deployed."
43;Speaker 2;00:24:31.730;00:24:56.490;00:00:24.760;"And so we made a very explicit decision that you can't trigger deployment programmatically. You need to do it through our service where you set up some rules, which was I think that worked really well for us. We're able to ensure that if we wanted to change how things are deployed or things how things are triggered, we only had to change our service. We didn't have to change anyone else's kind of pipelines or code or anything. So that was one very explicit decision we made. That's fascinating."
44;Speaker 1;00:24:56.540;00:25:23.270;00:00:26.730;"That's a very interesting decision. I feel like I can apply in a lot of different places, like a lot of MLS tooling, I think you say send or say deploy and making it so that you almost can't do that. It has to be integrated into the right system. It's been very opinionated. Yeah, that's fascinating. Well, I also want to talk about a number of big projects that obviously shared a lot about Hamilton. And maybe you could share more. What is Hamilton and how does it fit into the system?"
45;Speaker 2;00:25:23.370;00:26:18.450;00:00:55.080;"So Hamilton is a declarative data flow paradigm in Python. So what do I mean by that? It's declarative in there. As you're writing code, you're declaring an output, and then you're declaring more inputs. And so you're not writing procedural code when you're writing data flow. So what's the data flow? Data flow is basically an academic term for you're, basically modeling how data and computation kind of flow so you can think of it. And now I get to workflow a pipeline. You're basically building a workflow pipeline. And Hamilton, where it fits in and where it came from was it was actually our fifth one of the earlier projects that we did, and it was to kind of help the team who was doing time series feature engineering and the code base. It was like one of the oldest teams at Stitch Fix. They were a team that created forecasts, operational forecast for the business. That business made decisions on so they were always under the gun to produce numbers or forecasts so that the business can make decisions. And so which case they're not a team that had time to address tech debt or anything like that."
46;Speaker 2;00:26:18.560;00:27:21.860;00:01:03.300;"And essentially they were in such a state that feature engineering code was spaghetti code. And that's partly because with time series feature engineering, you are creating a lot of features. So you basically think of a data frame or data frame that you're going to be training or fitting a model on. It's thousands of columns wide. It's not necessarily big data, but it's very wide data. And it's very wide because of the feature engineering process, because you're usually deriving features from other features. So you're basically chaining features together to create other features. And that if you do it in a procedural way. I think once you get to a certain scale, your code can very easily evolve into spaghetti code, especially if you're using Pandas. And so Hamilton was built to try to mitigate and ensure that things are always unintestible. They're documentation friendly and it can kind of help them with their workflow. Creating features and generating this feature is data frame. So rather than writing procedural code, so back to Hamilton. So rather than running procedural code where you're doing column C equals column A plus column B, you would rewrite that as a function where the function name is C or column C, and then the function inputs, the input arguments, column A and column B."
47;Speaker 2;00:27:21.940;00:28:16.110;00:00:54.170;"And then you would have your logic to some column A and column B you could use the function doctrine to document it. And so anywhere that you basically have assignments, you're rewriting in a script, you're rewriting it into a function and then you then have to write a little more. We got a little driver script, but the driver scripts purpose is to basically stitch together these functions. So because the function name declares an output, if you want to actually create column C or use column C and some other thing, you would have to basically you write all these functions. We then crawl the Python code, the Python code in this kind of what we call a driver object to create a direct to the cyclist graph. So basically it's a dependency chain of like, hey, if I want to compute column C, I know I need to compute I need A and B as input, where an A and B, where A and B can either be defined as either functions. So you would look for a function name with column A, column B, well, that would be provided as inputs. And so Hamilton decouples the modeling of the data flow or the pipeline from materialization."
48;Speaker 2;00:28:16.220;00:29:10.690;00:00:54.470;"So you're writing these declarative functions that declare a workflow or pipeline or data flow of how data and compute move through. And then you're writing this kind of driver script that's actually defining the tag. And that's where you're either providing configuration and inputs and then at the end you're specifying what you want. Computer now, with Hamilton you can specify, or rather you can model a superset of transforms and things and only in the driver. You only have to request the things that you need. And because we have a Dag or directed at a point graph, we can kind of walk the graph in a way that we only compute what we need. So this means you can test integration, test things very easily. You don't have to have a monolithic script where it's like if you add something, you need to run everything to test something. Now, with Hamilton, it's very easy to just test that one thing you add end to end. It's also very easy to unit test because you write everything as functions. Those functions you're not leaking how data gets into it. So it's very easy to write a unit test that passes in the right data to exercise the logic."
49;Speaker 2;00:29:10.810;00:29:59.640;00:00:48.830;"And so that's been running a production statistics for two and a half years. Since then we open source in October. We are adding a few more things to it. So if you want to scale onto Ray and Dusk, it's very easy to do. So you don't have to do anything, you just have to change some driver code. And then we recently just added some basic ability to do some runtime data quality ticks. So common complaint was, hey, my pipelines are running. I think the code looks good, but the output is crap. What's going on right now? With Hamilton, it's very easy to set with the function just above it. With a decorator, you can kind of set some expectations such that when things run execution time, we can run a quick check to ensure types are there. There are no NANDs or if there should be less than 5% of Nan's and things like that. So I think Hamilton right now is a pretty interesting tool for anyone doing any feature engineering, and especially if you're doing time series feature engineering."
50;Speaker 1;00:29:59.730;00:31:02.890;00:01:03.160;"Yeah. One thing based off of listening to what you've been saying that I can have a very similar opinion on you is most of them lost problems, are abstraction problems, a lot of people trying to infrastructure problems. The hard part of ML Ops, in my opinion, is getting the right abstractions and workflow. You kind of have to be opinion some places in very configurable numbers. You have to say like, hey, we need to do things this way. That's only keeping versions organized, testable, et cetera. In other places it's like, hey, if you want to use in your example like Ray or Dash, then shouldn't matter. For MLS, just do whatever you want. Trained use. If you want to use TensorFlow, you want to use PyTorch, we don't really care because that's what you need is a data science. What we care about is all the coordination and metadata that comes and to allow you to do that without having to mainly say, hey, this is this and it goes here. We have to kind of set some parameters, some framework, so that you follow this framework and it's implied where it will go, what version it is, what the name is, all this other stuff that you want and that's pipeline."
51;Speaker 2;00:31:03.010;00:31:45.370;00:00:42.360;"Yeah, totally. I mean, I think that's like to prevent bad things from happening in production, you really need all the extra metadata. I think with the attractions it's like how do you get people to do it? So it's either the procedure, you have to add things in which case people forget, or they don't do it at all because of extra work. So then how do you do it in a way that can pull out the things automatically was part of the theme of my team is stitched. How can we do that in a way that people just don't have to think about it and it's the right way to do it. In which case Hamilton is a bit of a theme as well. Like you write functions, oh, they're automatically unit testible just by design. You don't have to think about it later, you don't have to like well, yeah, I definitely feel that it's an abstraction problem. If you want to do MLS well."
52;Speaker 1;00:31:45.490;00:31:54.380;00:00:08.890;"We talked about so much different aspects of MLS from surveying to data pipelines to how it's changed over the years. What's something you're most excited about in a mobile space right now?"
53;Speaker 2;00:31:54.950;00:32:51.880;00:00:56.930;"Yeah, that's a good question. I think there are so many open source tools, there's also so many startups and companies trying to do something in space. If I was a practitioner and I like to build models, I'd be excited because it has never been easier to kind of get something up and running without you having to build all of it yourself. Yes. I think to me, one of the things that I'm kind of, I guess looking to is like how does low code solutions impact MLS and how much code are people actually going to be writing to get models to production? Is it going to be config basis? I think, is it going to be integrated to a sequel? You see companies like, I guess the guys out of Uber, I think Ludwig, they're trying to use SQL like syntax to help people do models and do modeling. I think there's a bunch of companies like that. Then there's, I think companies that are trying to appeal to more people who want to write a little bit more code. So to me, it's kind of exciting to see all these different kind of approaches emerge and I'm kind of wondering who's going to win?"
54;Speaker 2;00:32:51.960;00:33:13.070;00:00:21.110;"Which one is going to actually ultimately went out? I wouldn't be surprised if everyone wins because there are so many ways in so many different companies that have machine learning but are structured and functioned differently and have different SLAs, in which case it could be one for all of them, but otherwise just be a Cambrian explosion. You could say I'm excited to see how things evolve as to what eventually dies off and what actually sticks. I love it."
55;Speaker 1;00:33:13.110;00:33:25.520;00:00:12.410;"I think there's so much more we could cover, but we do have to eventually wrap up and I guess I'd love to hear almost like a TLDR, what is the tweet link take away that someone podcasts lead with?"
56;Speaker 2;00:33:25.600;00:33:58.110;00:00:32.510;"Of course. Use Hamilton to take my course. Right? Yeah. I think the tweet length takeaways, I think understanding the environment of which you operate, that's not a tweet length. That's not tweet length. You should understand the impact on the cost of what you want to prevent. I think it's probably may be one way to frame what we've been talking about. So MLPs, I think, can be very specific to your environment. You want to prevent bad things from happening and so therefore the solutions that you want to kind of implement I think are related to your environment and what the cost is of bad things happening. So I think you should understand the impact and cost of what you want to bring."
57;Speaker 1;00:33:58.160;00:34:02.680;00:00:04.520;"I love that. Thanks so much for hoping on Devon. This has been such a cool conversation. Thanks so much."
58;Speaker 2;00:34:02.760;00:34:03.490;00:00:00.730;"Thanks. Have me simply."
