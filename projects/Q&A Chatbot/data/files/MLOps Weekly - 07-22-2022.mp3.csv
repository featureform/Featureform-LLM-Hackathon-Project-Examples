Number;Speaker;Start time;End time;Duration;Text
0;Speaker 1;00:00:06.010;00:00:25.880;00:00:19.870;"Hey, I'm Simpa Kather, and you're listening to the MWF Weekly Podcast. This week I'm really excited to be chatting with Nervous Date. He's a Forbes 30 under 30 recipient and the co founder of NCO of Obviously AI, which is no code auto model company. Prior to obviously AI Nerman built the AI infrastructure as streamlabs. Nervous, it's so awesome to have you on the show today."
1;Speaker 2;00:00:25.960;00:00:27.370;00:00:01.410;"Likewise. Thanks for having me."
2;Speaker 1;00:00:27.430;00:00:32.890;00:00:05.460;"I like to start by just learning a bit about your journey to where you are today. What got you to machine learning in particular?"
3;Speaker 2;00:00:33.010;00:01:32.090;00:00:59.080;"That's pretty interesting. When I was in college, I never wanted to do machine learning. I was very intrigued by data science and algorithms and really kind of going into the nitty gritty of it. One day we had an option to take a random course and you weren't going to get graded on it, so it's not going to affect your score. So for just shifts and giggles, I ended up taking a PhD level course on building neural networks from scratch using simply just NumPy. The entire course was about building neural networks purely from NumPy. I was like, I might as well do it because they're not going to grade me on it. It's not going to matter. So during that course, it absolutely blew my mind how neural networks function got really deep into things like activation functions, hidden layers, all that fun stuff, and really coded it from pure scratch. When I did that, it truly changed my entire perspective on how I think and be machine learning. And after that I ended up doing more machine learning courses and really became super into programming from that standpoint and really got into a bunch of different other machine learning field studies."
4;Speaker 2;00:01:32.160;00:01:37.830;00:00:05.670;"That's kind of how I just randomly got into it. It wasn't really planned, but it's pretty exciting so far."
5;Speaker 1;00:01:37.940;00:01:57.380;00:00:19.440;"I feel like knowing you for as long as I have, I'm not surprised that you went for the PhD level course. With that. I was like, cool man, I'm going to go do surfing. Obviously. We started the company. You're building Auto Amount. With all the huge range of things you could have worked on machine learning, what made you decide to work on auto mal?"
6;Speaker 2;00:01:57.470;00:02:57.410;00:00:59.940;"Great question. I used to work as a data scientist at a company called Streamlabs. As you mentioned, when I joined them, they were about CVS, they are about 70 people and I was the only data science person at the company. And so everyone that was not technical, right, like the sales analysts, marketing analyst, even entry level data analysts on the team, these guys would come up to me and they would say, hey, can you help me build a predictive model for churn or retention? And we kind of really go into kind of looking at which customers are active, which are new, which are kind of resurrected, and which are likely to churn and things of that nature. It was pretty important for Streamlabs at the time because the adaptive platform was built on influencers. If an influencer leaves Streamlabs streaming service, they're taking thousands of other followers with them. So these things became very, very important for Streamlabs then and I became naturally part of it. As I started to work, it was very clear that the need for AI and data science projects was there but the talent wasn't right. They had hired me, who was I was just kind of like some dude that was just in his college kind of doing that internship that hired me."
7;Speaker 2;00:02:57.510;00:03:57.220;00:00:59.710;"And it wasn't really about the budget or something but it was really the fact that kind of finding the right kind of talent with the right kind of industry background and domain expertise coming together that's really tough and kind of getting that streamlined is very tough. That's when it really started is can we build a tool that allows anyone that's a domain expert, that's an entry level analyst that's been in the industry for a bit without a technical data science background to build their own AI models, without writing code? That's when the idea of obviously I really started. We could have been obviously in multiple different directions. We could have focused on vision AI or natural language processing with a bunch of different things. But it was very clear from the beginning that majority of data science problems that exist today are mostly in kind of like very niche supervised learning, tabular data, that kind of focus. So that's why we decided to kind of double down on AutoML as our key expertise, kind of build it out from scratch and really kind of build something that was kind of breaking industry standards. Today we are the fastest, most precise tool in the market and that's something that we've always aspired to continue."
8;Speaker 1;00:03:57.360;00:04:29.210;00:00:31.850;"AutoML I've always found to be such an interesting space and your story we talked about was the story of pay analysts at a company, some data science. I can do a little bit of machine learning but AutoML would kind of give me superpowers, I guess for lack of a way to put it nowadays. Obviously that's still true in a lot of cases. But I'm curious about I guess as people progress and as now there are more machine learning teams, what's the future of AutoML? Does AutoML replace like all custom machine learning? They live side by side. How does it look?"
9;Speaker 2;00:04:29.310;00:05:26.710;00:00:57.400;"Great question. We like to see really what we do obviously in AutoML industry in general as more like a calculator for data scientists, right? Think of it as a calculator for accounting teams. It wasn't really necessary replacing any of the work that the accounting team did, but it accelerated it. So we really see that being very similar to accelerating data science teams or even like beginner level data scientists to do the work really quickly in terms of where we fit. It's pretty interesting because I have the privilege to talk to so many data analysts every day. We talk to at least 100. Either it's the users on free trial or customers. I get to talk to a bunch of those folks. And what we really learned is that there are two things that are happening that are very interesting to look at. The first one is what we call modernization of AI, which is probably what you've heard a lot in this space. But what that really means is back in the day or a couple of years back, companies really cared about things like proprietary AI models. Majority of that thought process has changed a little bit."
10;Speaker 2;00:05:26.770;00:06:29.970;00:01:03.200;"Most companies say they don't really care about proprietary. They care about something that can really accelerate and give them ROI very quickly in terms of AI models. So with AutoML, that's really very fits in, and it can very much coexist with other proprietary models that might be there, like face recognition or something like that. But majority of business use cases are really just there to kind of, how can we expedite them and how can we get to ROI quickly. And that's where we really see auto marketing very well. And the second one, as I talked about, is democratization of AI, which is the fact that companies don't have the resources to kind of bring on board PhD machine learning engineers and find the right kind of engineers that have the right kind of domain expertise. So what a lot of companies are doing today is they're just kind of getting these entry level analysts like swimwear me. And with AutoML, where it fits very well is that these entry level analysts now have the superpower of PhD machine learning engineer to do things very quickly. So that's how we really see automo putting in. It's not necessarily replacing any algorithms, but it can really coexist with custom algorithms or very, very proprietary algorithms that the companies might have."
11;Speaker 1;00:06:30.080;00:07:01.410;00:00:31.330;"I love your calculator analogy. I think that's interesting. I don't know if I've really heard it put that way before. I think that it's very interesting. It's also knowing when and where to use it and what to plug in. Into the calculator is the hard part. The actual calculation kind of just becomes like, who cares? Calculator. That's super interesting. Like you said, there's so many use cases for AutoML. I'm just curious, and you talked about how many people you've worked with. Could you share like a crazy story of someone doing AutoML for something you just could never consider?"
12;Speaker 2;00:07:01.580;00:08:00.260;00:00:58.680;"Hell yeah. We have so many use cases that we've come across. I'm going to try to see if I can name it for you. So first one is we work with this cookie company just like Insomnia Cookies, where they have hundreds of stores across the country. It's a very similar cookie company. They use this to predict how many cookies are wasted per flavor per day per store. Previously they were used to waste around like 36 items and now that it's kind of decreased to ten. And it sounds like a small number, but when you look at it at scale, that is little over 100,000 cookies wasted in the ingredients, that's been pretty exciting. We have an agriculture company that uses us to predict the yield of crops on a farm. That was never expected. Similarly, micro lending company using a SuperD credits risk and then a slew of staff company using us to predict things like churn retention, upsell, things like that. So very horizontal, very interesting use cases. But I think the one that's always taken me aback has been the agriculture one where they actually put in details about the fertilizer, the PH of the soil and things like that and use that to predict crop yield on the farm."
13;Speaker 2;00:08:00.290;00:08:01.810;00:00:01.520;"That's one of my personal favorites."
14;Speaker 1;00:08:01.930;00:08:05.050;00:00:03.120;"So you know what cookies they have left in every single store."
15;Speaker 2;00:08:05.170;00:08:06.620;00:00:01.450;"I can kind of tell that."
16;Speaker 1;00:08:07.850;00:08:34.580;00:00:26.730;"That's awesome. That's a cool story. I love that. Talking about democratization of AI or whatever, but cookie company using it to decide what cookies are out of stock, that's democratization of AI. That's awesome. Maybe you could share more about how the AutoML works, not so much like what ever, but more like do you have separate models? If I'm using obviously, do I just throw in my data or do I say, hey, this is time series data, how do I work? Obviously AI?"
17;Speaker 2;00:08:34.669;00:09:25.770;00:00:51.100;"Great question. So the way obviously AI works is anyone can bring their own data sets, multiple different data sources. You can connect that to obviously through spreadsheets or databases like Snowflake, My People, whatever you use, once you've integrated those data sources, you have the option to merge those data sources if there's a way you would like to merge them. And then from that point onwards, you can decide if you want to build an AutoML model that includes classification or progression, where you're predicting either a number or a text. Or you want to build a time series model where you're looking at how value has changed over time. So you can choose whichever route you want to take from that point onwards. You can either simply pick the values you want to predict for and go ahead and hit OK, and our system takes care of everything and or you have the ability to go really deep and say, hey, I want to autoimmune some of these values, I want to handle the upsample down sampling, I want these kind of hyperparameters to be there in the model. You have that option to kind of really dig deep if you want it."
18;Speaker 2;00:09:25.820;00:10:22.480;00:00:56.660;"But most of our users really quickly, when they're getting started for the first time, they're just kind of going through the simple process of saying, this is what I want to predict. Now, the funny part is, once that happens, the system automatically builds a model in decades. And that's why we are the fastest, most precisely in the market. And the way we do it is very interesting. It's very different from traditional AutoML structure. So traditionally AutoML structure says that you're going to go through thousands of machine learning models and you're going to pick one that has high accuracy and displayed to the customer. It's a brute forcing method and 90% of the time, 90% of those algorithms are useless, right? You're kind of just building them for the sake of just seeing if there's an optimization for a percentage increase in accuracy, things like that. It's kind of not that super helpful. So with other we use something called edge chart AutoML. What that means is that we actually look at the data set you're bringing in, the size of the data set, different properties like that. We look at the use case that you're accommodating for and then based on that, it's actually going to shortlist the top five algorithms you use."
19;Speaker 2;00:10:22.560;00:10:59.730;00:00:37.170;"So this could be a neural network, a random force classifier XE boost, whatever that is. And then it runs each of those algorithms with thousands of different hyper parameter combinations and picks one that performs best. That's why we can give you something that is extremely fast and higher accuracy than any of the tools in the market. The other thing is also that we have a strong focus on supervised learning on tabular data. We don't do any kind of deep learning, audio vision kind of thing. That's been a more strategic business decision that we've taken over years is to really focus on that supervised learning piece that allows our customers to have the best experience when they're using tabular data for their business. So that's kind of how we think about Autumn and that's how it works in the back end."
20;Speaker 1;00:10:59.840;00:11:18.520;00:00:18.680;"It must be really interesting for you from hiring and building this because you're kind of hiring data scientists to build the model that builds the model. How does your workflow look like internally? Do you back test on data sets you have? And also how different is it from a traditional process? Is it that different or is it kind of looks similar?"
21;Speaker 2;00:11:18.660;00:12:12.380;00:00:53.720;"It's pretty interesting because we're not constantly building AI models for internal use. We're not building models to predict something because to do that we just use our own tool. So the models that the data scientists are really building are models that are designed to be productionised to everyone, designed to be productionised for the users. So what they're really engaging most of the time on is saying, okay, there are a bunch of these model templates, right? Like a template for a neural network could be a simple neural network with five litlabs. Let's say that's a template, then they really spend a lot of time defining what are the hyper parameters that can be tuned and how does that search going to look like? Is it going to be a grid search, is going to be different types of search on the back end, what is that specific piece look like? And that's really what we're productionising. So when it goes to the customer, it's essentially an algorithm that is going to get customized for them automatically. That's where we spend a lot of our time. Our tech stack, however, looks very similar to what you would see in the industry, right?"
22;Speaker 2;00:12:12.400;00:12:19.850;00:00:07.450;"So we got Apache, Spark, we got Jupiter notebooks, TensorFlow, those kind of things happening on the back end. So that's where some of the data centers really engaged."
23;Speaker 1;00:12:19.920;00:12:29.460;00:00:09.540;"Did you have to build any custom tooling, like around maybe data sets versioning? I'm just curious, have you had to build anything custom to allow you to iterate as you've gotten more to scale?"
24;Speaker 2;00:12:29.550;00:12:38.600;00:00:09.050;"So not yet. We have plans of customizing some of the internal workflow that we're doing, but that's something that we haven't been hitting on just yet, so we'll probably be getting there soon."
25;Speaker 1;00:12:38.740;00:12:50.430;00:00:11.690;"So a lot of what you're doing, you're kind of taking things more or less off the shelf, like you said, using Spark, using TensorFlow, etc. You've been able to kind of build the systems you need to get that to work in this automatic use case."
26;Speaker 2;00:12:50.540;00:13:17.140;00:00:26.600;"Correct. I mean, it doesn't really make sense to kind of build TensorFlow from scratch or build new networks from scratch. Even though it was fun in that PhD class that I was in, it doesn't make sense on industry everyday basis. Where the real value comes in is the ability to automatically find best hyper parameter combinations, do it extremely quickly, and give the best ROI the best accuracy for the user. So that's kind of where we focus on the most, then kind of reinventing the wheel on the other side. Got it."
27;Speaker 1;00:13:17.220;00:13:26.190;00:00:08.970;"Well, the way it obviously works is there ever an idea of a real time model? Like the idea of the model is constantly deployed 100%. So you do deploy models too?"
28;Speaker 2;00:13:26.300;00:14:22.460;00:00:56.160;"Yes, that's correct. So kind of stepping back here, a typical analyst journey is about 60% of the time is spent prepping the data. That is kind of any kind of transformation work that you've been doing on the data sets, all that kind of stuff. Or about 30%, I'd say, of the time is there. 20% of the time is actually billing models. Right. We're trying out multiple different AI models, tuning hyperparameters, manually checking what works, what doesn't work. 10% of the time is arguing with the DevOps engineer on why it's not deployed correctly because something's going wrong. And all in all, it takes about six to eight months to get to anything in production. So the entire idea behind obviously I is not just to do some predictions and get back to you. But really to help you with that end to end processes. To say. Okay. Models are built. They are automatically deployed in a single click. And they keep getting better over time. Which means if you've integrated it directly with your snowflake or a database. It's pulling the latest data. Retraining the models continuously. Getting better. You've seen that version control, version change, accuracy change over the versions as well, things like that."
29;Speaker 2;00:14:22.480;00:14:38.030;00:00:15.550;"So I think that Peace is the most critical one because just building a model isn't relevant for most of our users today. It's really putting it to use in a way that they can seamlessly use it and that other people on their team can seamlessly use. It is where we really focus on that's awesome."
30;Speaker 1;00:14:38.130;00:14:44.730;00:00:06.600;"And for that kind of stuff as well. Have you had to implement any sort of MLS tooling like serving and what kind of stuff do you use?"
31;Speaker 2;00:14:44.840;00:15:00.510;00:00:15.670;"Yeah, so we use a bunch of different ML ops workflow on the back end. That's not my best expertise. That's going to be something that my CTO or tech team can talk most about. But as far as I know, these Kubernetes clusters kind of default all our models, and that's kind of what we do. That's awesome."
32;Speaker 1;00:15:00.620;00:15:18.450;00:00:17.830;"So continuing, when you're at Streamlabs, you mentioned, I guess a lot of your work was building these models in the production size. Nowadays, you've seen this across many, many different companies you've seen over the years too. What's changed over time? What's I guess happening in the market from your perspective?"
33;Speaker 2;00:15:18.560;00:16:19.690;00:01:01.130;"I think one of the major shifts that's happening is how clear has become in terms of how AI use cases have become more clear. Previously, a lot of the AI works that was happening in the market was very exploratory, where people were like, okay, let's kind of throw in a few algorithms, really see what it comes up with. Now it's like, we want to achieve X and how do we get there kind of a structure. So that's something that we've seen a lot. In terms of tooling, the important other thing that we've seen is previously a lot of data analysts talk about, analysts in specific really cared about upping their background and knowledge in machine learning code. So they'd be like, hey, I would want to learn TensorFlow, and I want to learn how to deploy these models. What does that look like? So they would spend a lot of time and energy over there. Now that has really shifted to saying, I probably don't need to learn TensorFlow. How can I learn how to evaluate the models best? I think that shift has really happened where people are saying, I don't want to get into the nitty gritty details of things, but I need enough knowledge to evaluate if whatever I'm doing is in the right direction."
34;Speaker 2;00:16:19.810;00:16:38.890;00:00:19.080;"I think those are some of the key changes that we've seen from doing my time in Streamlines, where a lot of people would want to engage in the nitty gritty. Now, an average data analyst at any company says, I don't want to engage in the nitty gritty. Like, I just want to get to this result right. And I want all the information in front of me if I need it. So that's kind of one of the key changes that we've seen so far."
35;Speaker 1;00:16:39.010;00:17:47.430;00:01:08.420;"Yeah, it's super interesting that you say it that way too, because a lot of people we've had on the show come from an MLS background, are more like deep data science background. Not as much analysts, but they're saying the same thing but from a different perspective. But it kind of follows that AI has kind of moved out of the lab, so to speak. It's like people are actually doing things in machine learning. It's almost like it's not boring yet, but it's like on its way to becoming boring. Like what Webdef was and like the.com boom where it's just like, oh, this is like the coolest thing ever. And now it's like I have to write CSS. No offense to any full stack people's names. Yeah, that's a really sad thing. You can put it that way, I guess. One thing that I found I built recommender systems before my triumph. We've met and when I was working on Recommended Systems, a lot of the work, what I did as a data scientist was taking my domain knowledge and trying to kind of inject it into either the features or the architecture of the model. And sometimes I move embedding both and I always felt like that was the goal, was the one thing you can't obstruct away is the domain knowledge?"
36;Speaker 2;00:17:47.540;00:17:48.210;00:00:00.670;"Absolutely."
37;Speaker 1;00:17:48.380;00:18:05.070;00:00:16.690;"But like you said, the calculator analogy, it's like once you have a domain knowledge, that's something that you can pick up and learn. As a data scientist, the goal is like, can I give you an interface so you can plug in that domain knowledge and it will kind of output model. Is that fair to think of it that way?"
38;Speaker 2;00:18:05.180;00:19:06.100;00:01:00.920;"Exactly. That's exactly where the AutoML space is today. Right. Going back to the calculator analogy, which is an accountant still needs to know how a profit and loss sheet works or how a balance sheet works, but they still need to know what to put in and how to really differentiate the line items and take it from there. So they still need to know those details. They still need to know your business to truly, really understand the details of all the work that you're doing. Only the calculation part becomes so streamlined that it's just an extension of their own self. And we're seeing very much similar with, as you pointed out, data analysts today, the data analysts still need to have that domain expertise. Right. When you come in and you bring in a data set. We had a company we work with that was doing loan repayment predictions and it took a lot of domain knowledge to really define what are the stages of loan repayment is it repaid, is it defaulted? And then there are multiple steps in the middle that come through, but that can only come to domain expertise. Once you have that, then you can run a classification model really quickly to kind of a test, what kind of users are coming in, what is the predicted default rates, things like that."
39;Speaker 2;00:19:06.180;00:19:22.780;00:00:16.600;"But the ability to kind of really thoroughly think of how am I going to put this data together and how am I going to evaluate the outcomes. That does require the main knowledge. And that's where the accountant analogy comes in and the calculator analysis comes in. So very similar to what you're saying. That's really where the world is going towards today."
40;Speaker 1;00:19:22.920;00:19:25.970;00:00:03.050;"What about the amount? What does the day of an analyst look like?"
41;Speaker 2;00:19:26.070;00:20:17.810;00:00:51.740;"Great question. So day of analysis is typically, let's say they come into work and they're going to find a project to kick off, let's say loan repayment prediction. One of the first things that are going to sit down and do is they're going to look at, OK, where is all my data set? What all data do we collect? What are they, do we have, what are they that should we be putting into the model? Do I want to put in demographics, like agenda, location, income, we want to put in other things into the model. That's one of the first decisions that I'm making. Second thing that they're working on is really saying how do I pull this data together and how to bring it in a place that's going to be really meaningful for me. That's kind of one of the key places that can be spending time. Then they're actually defining different ad tests. They're saying, okay, let's actually build a model that's going to include age. Let's build another model that's not going to include age, but it's going to include location, things like that. So defining those Amy tests becomes really important for a day in their life because that's what we're going to then define the business outcome that is looking for."
42;Speaker 2;00:20:17.910;00:21:14.750;00:00:56.840;"And then with AutoML, they run all of these models extremely quickly and obviously they're building these models in a minute. They're seeing the results. Then they spend a lot of time evaluating the results, looking at precision recall, F, one score area on the curve, seeing if all of these things are kind of aligned, looking at the confusion matrix, making sure that there are less false positives, false negative. Kind of really evaluating the models itself, trying out different models. Let's say obviously I automatically pick the Xg boost, but they're like, no, let me see if there's a different result with a random forest that can come up. So those are the kind of things that they really engage in on day to day basis. And then they say, hey, finally, after doing all these tests, this seems to be the model that works best. This is the model that, let's say, doesn't have certain features or these are the tweaks that we've done. This is the test that we have set up that seems to work. And now let's kind of productionize it. So that process going from here's a problem to what data to use, to really building out tests and saying that the models are the best models."
43;Speaker 2;00:21:14.820;00:21:17.420;00:00:02.600;"Those kinds of decisions are what they're making on a day to day basis."
44;Speaker 1;00:21:17.570;00:22:00.450;00:00:42.880;"Yeah, that's super interesting. I think it's also interesting. One thing that we've run into with users of ours that use AutoML is they start to see features and the data as the domain knowledge. That's where you can inject your domain knowledge. And so what you're describing is very similar. I mean, there's kind of two parts to it. One part is the domain knowledge, what really makes sense. The other part is organizational knowledge. So much work goes into just like, what data exists? Where is it? How can I use it? How do I even get it out of SAP so I can fit it into whatever kind of how you think about the space too. Do you see kind of the features as being the part that the data analyst kind of that's their ownership. That's what they try to do 100%."
45;Speaker 2;00:22:00.560;00:22:20.910;00:00:20.350;"That's probably where I really love the direction that you guys are working on, is really that part of really thinking through the features becomes critical for an analyst journey. And really, again, like an accountant thinking through the line items. What is the business, what are the line items that matter became important very similarly for an analyst. Is the features 100% happy?"
46;Speaker 1;00:22:21.020;00:22:38.720;00:00:17.700;"That's awesome. I feel like I could talk to you all day about AutoML. I'm sure there's so, so much more. I could continue to learn about it. But for people listening at home, if you have to give a tweet late take away about OML, if someone go back and be like, hey, this is the tweet that encapsulates this podcast, what would they write? What do you think?"
47;Speaker 2;00:22:38.800;00:22:42.850;00:00:04.050;"If you're a modern data analyst, AutoML is your new calculator."
48;Speaker 1;00:22:42.970;00:23:00.470;00:00:17.500;"I love that. Yeah, well, it's on purpose, but it really struck me. I really like the analogy. Nervous. It's so awesome. Have you on it's great to catch up. I love what you're working on. I'll have some links so people go check it out. Back to her prop and sharing more about not only AutoML, but also how it works under the hood today."
49;Speaker 2;00:23:00.570;00:23:16.780;00:00:16.210;"Absolutely. Thank you so much for having me. It's you."
