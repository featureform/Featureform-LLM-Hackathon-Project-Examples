Number;Speaker;Start time;End time;Duration;Text
0;Speaker 1;00:00:06.010;00:00:20.650;00:00:14.640;"Hey, I'm SIM Mccauder, and you're listening to the MLS weekly Podcast. This week I'm chatting with David Stein, the tech lead of the ML feature infrastructure team at LinkedIn. Most recently, he's been working on and recently open source Fabric Feature Store. David, so good to have you on show today."
1;Speaker 2;00:00:20.770;00:00:22.530;00:00:01.760;"Thank you. Simply, it's good to be here."
2;Speaker 1;00:00:22.640;00:00:28.730;00:00:06.090;"Maybe we can start by just talking about your journey. Zamolo, what got you into how did that kind of journey look like?"
3;Speaker 2;00:00:28.890;00:01:38.530;00:01:09.640;"So, I've been at LinkedIn for about ten years. Pretty much the whole time I've been working on problems relating to Mlaps. We didn't call it ML Ops back at that time. The team I joined when I was new at LinkedIn was focused on the recommendation systems, the recommended system stack, and experimentation stack. As you probably know, and probably many of the listeners know, LinkedIn has a lot of use cases of machine learning in production to improve the personalization, the quality of a lot of different aspects of the product. And I was working on this team as a new hire that was building a bunch of the different recommender components. And there was definitely like an awareness going back ten years ago that there are a lot of common pieces and a need are generally utility of having good abstractions to try to minimize the redundant infrastructure kind of work that is needed in order to maintain and support any one of those kinds of vertical areas. Now, that's probably kind of obvious stuff to most people who are thinking about ML Ops now, but I've really been just working on these kinds of projects to support multiple different recommender and ML systems over this time at LinkedIn."
4;Speaker 2;00:01:38.650;00:01:54.570;00:00:15.920;"It's been a cool space to be in, definitely with all of the increasing interest, I guess, in the industry, in this space around how do we streamline putting machine learning in production, what kind of tools should be in place. That's kind of how I got here."
5;Speaker 1;00:01:54.740;00:02:15.020;00:00:20.280;"So you mentioned how you've been doing MLOps since before it was ML Ops. That makes a lot of sense. I mean, Missionary has been around a lot longer than the term MLOps has been around. It's been production a lot longer than the term amongst has been around. What stays consistent, what has remained kind of the same. And how you think about ML ops, how you think about structure over your career?"
6;Speaker 2;00:02:15.170;00:03:29.750;00:01:14.580;"Good question. So what has been consistent, I would say, is a lot fundamentally, I think that machine learning development, applying machine learning to solve business problems. For a company like LinkedIn, it's a very experimentation driven process. It's like we have teams of researchers and engineers searching for the best algorithm, the best models and feature engineering feature definitions, searching to be able to find the best algorithm to do the best possible job at helping LinkedIn members find job postings that are a good fit for them, or helping people find relevant search results or these problems. So this search, we may be able to search the space efficiently. And I think that there has been an awareness for a while about the sort of obvious importance of making that efficient as possible, to be able to productively search the space of solutions, to be able to try new things, to be able to try new things effectively and safely, be able to get them into production and be able to understand the impact. I would really say it's about being able to try things easily and also being able to understand the current state so that you know what next thing is, Bill to try."
7;Speaker 2;00:03:29.850;00:03:35.330;00:00:05.480;"There's like a lot more I could say. If you have any questions about what I said, you can go in any direction with that."
8;Speaker 1;00:03:35.430;00:04:01.420;00:00:25.990;"Yeah, one thing you've said, actually a few times, you've said I'm a lost on infrastructure. And you kind of have a distinction, I think, how you think about them, which I think I do too. We made up the term virtual feature storage, I think, or like that whole term exists around the whole mop idea as opposed to the unfit structure idea. It kind of creates a separation. How would you define maybe first MLPs and then ML infrastructure, in your own words?"
9;Speaker 2;00:04:01.620;00:04:36.760;00:00:35.140;"Sure. So I see ML ops to be about operations, the ability to move models to production. So similar to how I think about things like CICD pipelines for general Software Engineering and development. I think that we could probably describe subdomain of machine learning infrastructure and tools that are specifically focused on being able to try things quickly, safely put models in production, define new features easily, readily predictably, reliably with transparency into their metrics and things like that."
10;Speaker 1;00:04:36.960;00:04:58.760;00:00:21.800;"Yeah, I think another way to say it, I think you've been over getting it. I'm the opposite of the workflow part and infrastructure. Maybe it's like the underlying all the tools that kind of come into play, like the serving infrastructure and the training infrastructure. And that's where the CITV analogy maybe comes in. Citizens do anything by itself. It kind of coordinates everything."
11;Speaker 2;00:04:59.090;00:06:10.010;00:01:10.920;"Yeah, I would say that that's right. Practically speaking, we've been able to benefit a lot from tools maturing like basic ML infrastructure tools and frameworks and platforms that let you define author models and machinery for training models. Over the span of time that I've been talking about over the past ten years, we've seen things like TensorFlow and PyTorch become prominent. I think the kinds of infrastructure pieces that we invest in now have probably changed a little bit because we're not really trying to reinvent things like that where good standards have really emerged in the industry. Instead, it's like kind of like you were saying, like integration, making it so that people can do their experimentation and then take it and evaluate what it's going to mean to run this in production and then safely try running it in production and have that kind of CI CD, but for machine learning development pipeline built around those kind of standard golden set of industry tools. So I think that to some extent, the game has changed in the past few years. It's less about inventing an entire set of tools and more about finding the good ones that are really built."
12;Speaker 2;00:06:10.050;00:06:34.220;00:00:24.170;"A lot of momentum around themselves. That are well understood in the industry. That are really well built and employed at many of the biggest places. Are also well plugged into. Like the education system and the people are using in their graduate programs and things like that. And building the kind of CI CD pipeline to make it so that you can put those tools to use efficiently. Effectively for that kind of ML prospecting."
13;Speaker 1;00:06:34.370;00:07:33.020;00:00:58.650;"It's so interesting how the DevOps versus ML Ops analogies, they come up a lot like, CI CD, obviously is a great example of this. It's something that I talk about a lot. I think that there's a lot of even beyond just a problem to be solved, it's kind of a similar problem applies to different people. There's also even the markets, which I won't quite get into in this conversation yet, but even the market looks the same. Like in the early days of the Ops, it was kind of just everyone built their own pins and house. Like Bob exists today at Google. Like all these people just built everything from scratch because that's what you did. And then there are some standards that got built in the early days. It's Wild West, there's a billion tools that people use. And now it's like, yeah, you use Kubernetes, use TerraForm, you use whatever CI CD tool. Like, it's kind of finally become a little more standardized. And one big difference though, between DevOps and ML Ops is experimentation, like you said. And DevOps, it feels like you kind of know where you're going. It's rare that you're just like, I'm going to try this and see if that works."
14;Speaker 1;00:07:33.040;00:07:40.350;00:00:07.310;"And if that doesn't work, I'll try this other thing. It doesn't happen as often. It definitely doesn't happen. It's not a constant part of the process. Is that fair?"
15;Speaker 2;00:07:40.460;00:07:40.760;00:00:00.300;"I guess."
16;Speaker 1;00:07:40.780;00:07:52.570;00:00:11.790;"How do you think about the difference? Maybe? What is your view of the difference in DevOps and ML Ops? Why are they different fields? Why don't we just expand our CI CD to just have them all trained them?"
17;Speaker 2;00:07:52.690;00:07:59.240;00:00:06.550;"Yeah, that's a really interesting question. So you mentioned that with DevOps there's a difference in terms of the focus on experimentation."
18;Speaker 1;00:07:59.390;00:08:28.940;00:00:29.550;"Yeah, it's like ML Ops is much more experimental. It's like a lot of what I'm doing as a data scientist is exploring data, trying different models out there's a lot more, I guess, random walks, like a random walk algorithm, and just more research, I guess, in the data science side, like DevOps. As a developer working on software, I kind of know what the next thing I need to build this? It's rare. I'm like, well, I'm going to try to build this feature and I'll build this feature and I'll build this feature and see what happens."
19;Speaker 2;00:08:29.510;00:09:37.150;00:01:07.640;"That is interesting. I think that that is true to an extent, my understanding of DevOps and at least like the tool. I'm most familiar with the machinery that's within LinkedIn because I've been at LinkedIn for a long time. I do see this being to some extent about kind of experimentation. Not exactly in the same way that you do in order to decide what direction you're going to go on your ML project. But there is a system in place where you make modules prove that they are safe to deploy by validating it with metrics and measuring what happens when you deploy it just on one piece of the cluster. Things like feature flags that exist in another space where you're able to like I don't know if this is really considered CICD, but I kind of consider all these things in like the same kind of group of things that lets us manage changes. There might be a risk that a change is not safe. It may cause some failure when things are pushed into production. So having automated testing machinery that makes sure we can safely evaluate how that works and that we can understand and push a button and revert it to a previous safe state, those kinds of not fully trusting any change that's about to be made need to be able to measure some things about it before deciding to put it in there."
20;Speaker 2;00:09:37.270;00:10:33.750;00:00:56.480;"I think that a lot of those pieces are maybe less kind of experimentation in one sense, but still kind of need to like measure and validate and be able to before pushing things out in front of everyone are something that's actually, from my point of view, to a large extent, common. There are definitely differences, I think, a lot about not just like DevOps tools, but other tools that we use to solve various kinds of productivity challenges in software engineering in terms of trying new things or moving them into production. There are, I think you said, like a lot of analogies that we should as an industry be exploiting more. What are the lessons that we can learn from how collaboration problems or rapid artifact deployment or things like that? Good solutions that have already been built for software engineering. Taking those lessons and applying them in the ML domain is sort of a big part of what Mlaps is, in my opinion."
21;Speaker 1;00:10:33.920;00:11:09.610;00:00:35.690;"I think there's so much here still to unpack and I think a lot of it just hasn't really been figured out in the space yet. I think there's just a lot of open questions and I think the reason that's true is I don't know if anyone really has a perfect ML Ops workflow. Even in DevOps, even in the early days of DevOps, people are like, well, this is what Google does, so we'll do it like that. Or this is what name any top technical company does that is like the gold standard. And in ML I've worked with and talked to a lot of companies and I don't know if I've ever met anyone who's like, yeah, we do it perfectly here, we got it down."
22;Speaker 2;00:11:09.670;00:11:10.810;00:00:01.140;"This is how it should work."
23;Speaker 1;00:11:10.930;00:11:35.850;00:00:24.920;"Everyone should kind of do it like us. Not even from the ego point of view, but don't begin with even close to that yet. A lot of people are like, yeah, we're still very in the early days, it would be really interesting to understand, let's say a data scientist at LinkedIn working on machine learning. How does their workflow look like? You talk about the average of them, but also I'd be interested to also understand where they interacting with, like they use Data Hub or Fever or something else. Like understand features and data that exist."
24;Speaker 2;00:11:36.020;00:12:51.500;00:01:15.480;"Good question. So things that I can comment publicly about. There are some blog posts that our machine learning infrastructure team has released over the past few years about our kind of productive machine learning and machine learning infrastructure initiatives. So I encourage listeners to go check that out. Just if you search for LinkedIn Pro ML, like Pro Hyphen ML, sort of like a project that we publicly described before, and blog posts that talk about some of the aspects of these things. Definitely the pillars of this are around giving users who are linked to engineers working in the ML applied ML space, giving them tools for exploring data sets and modeling, or trying to run modeling ideas and feature definition ideas, and being able to do that with open standard tools like TensorFlow is one example that LinkedIn is definitely using heavily. I'm pretty sure that's publicly been described. Moving along in the workflow, going step by step, it depends on, I guess, what you're doing. Like if you're developing a new model pipeline for a new problem versus iterating on an existing one, the workflow could look a little bit different. But I guess the notional thing is that you are able to explore data sets."
25;Speaker 2;00:12:51.590;00:13:57.610;00:01:06.020;"LinkedIn does have tools like Data Hub, for example. Data Hub, of course, is an open source solution which originally came from LinkedIn. We have solutions like this. We have heavy use of Spark. We have model training workflow definition systems that are in house solutions that I think are described in some of these public engineering blog posts that our engineers are able to use in order to define a training pipeline, identify which features from the Feature Store to pull in. Feature store is obviously a whole other piece here. And be able to define what the features are in terms of the raw data sets that the company has anyway. It's going to be able to run the model. Run the training. See the metrics. See the impact. Find an idea that you want to deploy because it looks like it is having a positive benefit and then use the other parts of the workflow and other parts of the platform which are LinkedIn model deployment and registration systems that push the model into LinkedIn production environment for running an inference. And there are more details I could give on some of those areas. I obviously have the most focus on the feature definition and production analysis, feature exploration kind of side of this."
26;Speaker 2;00:13:57.730;00:14:04.850;00:00:07.120;"Yeah, that is kind of the high level workflow kind of vaguely described and folks can look at those blog posts to see a few more details."
27;Speaker 1;00:14:04.980;00:14:21.440;00:00:16.460;"Is there any design decision that you all made on the ML platform? If you can't share it on the whole platform, you could just be about fever, but maybe just a design decision that was made that you think is unique, like it's just something that's maybe uncommon that really works out for you."
28;Speaker 2;00:14:21.770;00:15:47.680;00:01:25.910;"Thinking about whether to try to answer this for the whole machine learning platform or to focus more on my sort of subdomain around the feature infrastructure in the Future Store, I'll talk probably first about Feather and we could talk more about other aspects of ML platform. Maybe if I come up with top ideas, I'd like to share more generally something with Feather that we prioritized. In the early days, kind of a design principle was focused on collaborative machine learning, feature definition, the ability to share work. So we didn't just want to have a system that lets you define some features and ship those to production and use them. We wanted to be able to define features based on raw data sets in a way that the definitions one engineer writes can be used right alongside definitions that other engineers write. That may sound like a simple or obvious thing, but there's a set of low level, important details at the core of how Feather works that required us to lean hard on that assumption in order to be able to efficiently, for example, only load certain source data sets once. Even if many different engineers decided to base feature definitions on those common sources, it required us to make design decisions and the details of how Feather works to kind of allow it to be this collaborative system."
29;Speaker 2;00:15:47.880;00:16:41.050;00:00:53.170;"And it would have been hard to try to add that on as a feature down the road. So we leaned in pretty hard on that and I think that paid off because it is one of the cool things that we provide. Like that you can use feature definitions that go against common sources. The definitions don't need to know anything about each other, they can be written in different places, they can be consumed and used in a commonplace, computed efficiently together in a way that kind of facilitates this collaborative setup. And it also enables simple feature definitions. It's not as though, like every feature that's defined on certain data sources needs to be jammed into one place. So there are other details like that that I think have led to a better ecosystem than we would have had if we didn't focus on that as a principle."
30;Speaker 1;00:16:41.230;00:17:24.319;00:00:43.089;"I wanted to dig into it. We both know futures very well, so I can kind of assume what you're getting at. But just for people who maybe don't, in our case, one thing that we're leaning into is immutability. That's one where we're able to do feature sharing and other things. You mentioned, like, one thing you decided and designed the API is that, hey, feature sharing, that's like a first class thing that always needs to be possible to do, and it's not something that can be added. Like you said, you have to have this as a priority from the beginning for an implementation to be possible. Otherwise there's no way to kind of do it without a full rewrite, more or less. What are some of the things you actually did to actually allow you to be able to implement feature sharing? Maybe what did you have to limit? Could be one way to think of it, sure."
31;Speaker 2;00:17:24.950;00:18:34.310;00:01:09.360;"The first thing that comes to mind is that Feather actually controls the loading of the underlying data. It doesn't put into the hands of the engineer who is writing the feature definition the ability to use Spark API's to load the underlying data directly. Users specify the location of the underlying data, but then they're subject to some API constraints that Feather provides in terms of defining what kinds of transformation aggregation operations can happen on those data. But it's not as though the user is able to provide the loading of the data and the transformation of the data all in one bundle. At least. I say that a little bit. Simply saying you load the data in one place, but you can actually manage that independently and then run all of the related pieces of transformation logic that may have been written by various engineers. So this is like a limitation in the API. This was from the early days. This may give rise to other questions. This is like painting with broadcast. There are some details there that may kind of be slightly different from the main rule that I've described, but hopefully as like a general rule for designing a platform."
32;Speaker 2;00:18:34.410;00:19:23.480;00:00:49.070;"There are certain things that you need to be careful about if you want to be able to apply just API design or abstractions design, forcing the specification for transformation to be defined separately, or at least in a separate UDF or in a separate definition module compared to where the data gets loaded. It's just like one example of something that enables that. It's like one small thing. There are other things in terms of we built machinery early on in order to allow teams to import feature definition modules that were written by other teams. We basically built that capability early on in the lifespan of the project and trying to think what other lower level details here or even higher level insights to comment on the question, but hopefully that's making some sense. Or I don't know if you have any specific angles of it you want to go further into."
33;Speaker 1;00:19:23.810;00:20:12.300;00:00:48.490;"I actually love to zoom back out now. I think one thing everyone can take from that, so if you're building a feature store team, is, I feel like APIs. A lot of the hard parts memo ops nowadays. I think it's API problems. How do you design an API that actually works? Like implementing the API? It's hard, obviously it's not easy, but I feel like that part is just more understood. I think you can get a team of smart data engineers, involves people, and you can kind of build anything but deciding what the right abstraction is coming up the right API and every API comes with custom benefits. There is no perfect API. There's always going to be some trade off. What do you think of it? Is that a fair way to think about, I guess, building platform tools?"
34;Speaker 2;00:20:12.450;00:21:20.300;00:01:07.850;"I mean, I definitely agree with that. Defining right APIs is huge because it's related to defining the clean concepts that people are supposed to work with in order to use the system. I personally think that a lot of the biggest innovations and breakthroughs that have been the most important things for big data over the past ten or 20 years have been related to APIs that really got it right. Things like MapReduce being a really obvious example. It's like that's basically an API actually previously existed, obviously, because functional programming and all these fundamentals. But the application of a good API that lets you cash out the problem in a specific way can really be extremely powerful, right? Because it can help people think about how to formulate their problem in a way that a good platform can actually serve them. So finding the right abstractions, the right APIs is the name of the game, right? I could go a little bit into one facet of this that was important in Feathers design, and that is the idea that may even seem obvious now, I guess, like now that we have feature stores and we know what they are and we talk about them."
35;Speaker 2;00:21:20.380;00:22:30.710;00:01:10.330;"But I would say that early on, when we proposed and built this out originally, it was not a settled, obvious idea that a feature should be an abstract kind of like an entity, a concept that should also exist outside of a specific model, for example, or a specific feature transformation program. The idea that a feature exists as an entity outside of the big data data lake environment, or outside of the online system that it exists, we could have a registry for them, that they should have names and they should have tights, and we should be able to reason about them separately from the code that produces them. I think that's sort of like an innovation that many of us collectively kind of figured out while trying to solve the problems that we had at these businesses in terms of being able to organize the work. We had smart engineers designing features before that, but it was like entangled in the model logic or other parts of the data pipeline. So the kind of core piece in Feathers conceptual model for features is it's like you define named features, and then you get the features by their names."
36;Speaker 2;00:22:30.840;00:23:08.360;00:00:37.520;"In the right context, you define features that are named features, and then you get those features by their names. And that was like the simple kind of abstraction, like API basically. It's really sort of like a meta API, actually, because then you have a couple of different APIs that actually make that happen in different contexts. One for training and one for both the generation and one in the online setting for Entrancing and stuff like that. But the heart of the abstraction, that was sort of like the nut of it, right? Like, define features that are named, get those features by their names. And that's a big part of what makes feature stores useful, is giving people the platform to cut this down in that way."
37;Speaker 1;00:23:08.500;00:23:53.440;00:00:44.940;"Yeah, I love that. Everything I found when you come up with a good design MapReduce is a great analogy for this. All these sorts of use cases that you didn't think about start to kind of work naturally. I never thought of that. But yeah, it just works by design. Like, the design is clean enough where the way you're logically thinking about, let's say a feature is so well encapsulated by this API, I'll give you an example. One thing that came up quickly for us was access, control, governance, all this stuff. Having that be like a first class part of a feature. You can actually set not just table wise governance, row wise or whatever. Like, you can actually set feature wise governance best possible."
38;Speaker 2;00:23:53.520;00:23:53.930;00:00:00.410;"Now."
39;Speaker 1;00:23:54.030;00:24:58.650;00:01:04.620;"Now, to be honest, we built a set. Our last company weren't exactly an enterprise that really had amounts of requirements that a bank would need, but because the traction was so clean, it was very easy to kind of layer on top all these other things. And then when it's come up recently, I'll share the same thing. We have the separation of the application layer, like the feature store application and then infrastructure. One thing that's come up is, hey, you can actually have different clusters. Like, you have a spark cluster in the US. And the spark cluster in Europe, or hey, we're using Cassandra here and Redis here. Not something that when I was designing a lot of this stuff, I was like, this is a use case that's going to call up a time. We should definitely design for it. It just worked out that by design I think there's a lot to be said and I think the thing a lot of people miss because it's so much easier to every single time you kick comes up, oh, we can build that. You build it in. You build it in. You build that really opinionated, deeply bolted together platform that works if you use it exactly as designed, but doesn't really have any flexibility."
40;Speaker 1;00:24:58.820;00:25:39.530;00:00:40.710;"And I think the good thing, like CSV going back to DevOps is some of the best tools there. TerraForm is a good example of it. They're pretty generic if you can kind of almost abstract things like this. Generic, it's a conflict file that creates metadata and there's a coordinator that makes it. So that's actually kind of a description of a feature store in a funny way. But that concept, the traction, the style we took, the architecture, this works so well that your bank can use TerraForm. You're working on a small project with your own AWS cluster. You can use them anyway. I think there's just so much to be said here, and I think that people don't start with the API, they think about functionality, and I think that's a huge mistake."
41;Speaker 2;00:25:39.660;00:26:48.590;00:01:08.930;"Yeah, clean concepts and APIs are really essential. And I like your story that totally makes sense about having a registry of named features. Gives you a foundation on which to add things like you mentioned, like governance, support and things like that, that would be very difficult to just sort of implement in an ad hoc way if you didn't have the benefit of this named abstraction. That should be like a tweet or something. Names are really powerful. Having a system of naming things is sort of like the foundation to be able to organize them in a meaningful way and be able to do things with them. Names, symbols, Identifiers, they really matter. They're very important things. When I think about feature store and Feather, I tell people sometimes, you mentioned TerraForm, but I also think there's an analogy to how package management works for software build. Right? Being able to define the dependencies of your module on whatever packages are required and whatever versions are required, that is like a foundational piece of being able to do software engineering. And it relies on the idea that we all take for granted, which is that these artifacts actually have names and that they exist on their own as entities."
42;Speaker 2;00:26:48.660;00:27:52.620;00:01:03.960;"Someone needed to invent that going back like computer engineering classes, having symbol tables and linkers and loaders and libraries with names. I should learn more about the history of computer engineering. I'd love to read about the folks who actually invented the library abstraction for compilers and linkers and stuff like that. Because if you couldn't do that, first of all, it's not totally obvious, especially if, you know, like machine code, things like that weren't at low levels, like at the level people were dealing with decades ago. The idea that. These modules should exist in named entities called libraries that have contracts, I assume based on what I understand is revolutionary because it lets you actually manage these relationships and link things together without needing to do it manually. So I try to imagine what it might have been like to work in software engineering 40 years ago. Two laboratories on the same street probably have different architectures, different ways of writing code. It would have been impossible for them to share a library. And that is how I have seen a lot of machine learning development over the past several years. It's like if you have totally different systems, you don't have a common set of names for things like features or registry for features and models, you cannot collaborate."
43;Speaker 2;00:27:52.710;00:28:07.410;00:00:14.700;"It's hard to even collaborate with yourself. Like what you did last year may be really opaque now. So entertain collaboration, crossborder collaboration relies on named concepts that are well organized in a registry. It's like a foundational piece."
44;Speaker 1;00:28:07.580;00:28:46.310;00:00:38.730;"I love that I worked on the Astrophysics Project, but I was back in school and had to learn some poor trans. And I can tell you, it was not pretty. We take a lot of things for granted nowadays, and it's funny that we're always learning. We always look back and we're always like, man, that was really silly. I can't believe that's how we used to do things. It's so obvious. We should have had Python back then, but back then my fortran was like a step up from writing assembly directly. So it's so much here. I want to make sure that we talk about Fever. Obviously, you guys open Source Fever, the feature store that LinkedIn built and uses probably have gathered by now. But tell us about Feather."
45;Speaker 2;00:28:46.500;00:30:00.750;00:01:14.250;"Yes, so Feather is LinkedIn's feature store solution. We have a blog post about it describing the problems we're solving. I can briefly give the quick explanation. A lot of it probably would have come through and some of the stuff that I've been saying previously, but before Feather, before feature store and feature register abstractions, we saw a pattern on many different teams where feature preparation is like the most complicated part of the workflow for developing a model. You have huge workflows that kind of gets more complicated over time. The more researchers and interns or whatever would join a project and add pieces, you know, people look for. Where can I insert a little bit of extra data here? How can I add another facet there? The systems are hard to keep organized over time without common named registry for things. And so in order to avoid these different applications or having to have all the management of feature preparation, we replace the need to do most of that stuff with having a kind of common abstraction for defining features and then getting the features by their needs. And we deployed this. There was a talk about it a few years ago."
46;Speaker 2;00:30:00.800;00:31:17.380;00:01:16.580;"I think 2018 or 2019 the ML platforms meet up. I think there's a link to the slides for that talk on our blog post. But Project had a different name which was trained at that time. We built this thing, deployed it for many of the LinkedIn machine learning use cases and kind of optimized various parts of it over time for various large workflows. And we have seen that there's been a big interest in feature stores and feature engines in the industry and we thought this would be a cool thing to put out there in terms of showing the great work that we do at LinkedIn and potentially like collaborating with the community and we have been able to get some exciting traction there. We have some colleagues at Microsoft and the Azure team who are working with us. They wrote a blog post also about this collaboration where we have made Feather to be like Azure native support thing with very nice how to get started guides on how to get started playing around with Feather on Azure. So if you go to our GitHub page, Feather spelled Fe A-T-H-R with no second E in the word, you can find our GitHub page and you can get started with that stuff and we definitely welcome any input or anything from the community."
47;Speaker 2;00:31:17.520;00:31:24.030;00:00:06.510;"It's been exciting to see what folks have said so far. It's an exciting day for this project where's ever headed."
48;Speaker 1;00:31:24.350;00:31:29.890;00:00:05.540;"I guess it could be like the next big thing you said about whatever. It could just be the long term vision for it."
49;Speaker 2;00:31:30.010;00:32:54.470;00:01:24.460;"This could be like a huge topic. I think that the things that I most want to see solved. Well right it's sort of like what you mentioned going from Fortran to Python, seeing how far we've come, but yet there's still room to grow in terms of making the tools that we use even better and clearer and solving the next round of issues and finding next round of opportunities to make these things better. I think that now we have these foundations of this named feature registry and the ability to define features based on raw sources and the ability to do in a collaborative way. There are other pieces that we have some support for but that is going to I think need to enrich the support for these things, especially around making it easy for any use case that involves a highly dynamic or real time feature. This is something that I watch and look at various solutions that support this to varying degrees. Filling in the support for real time signals I think is going to be really important in the industry. Having some level of support is I think where a lot of tools are at today but kind of where I see feature stores going and I think a direction probably is going to go is solidifying that kind of support so that it becomes very easy experimental workflow that a person can follow to try a real time feature tracking something and be able to get that ultra fresh."
50;Speaker 2;00:32:54.660;00:33:30.030;00:00:35.370;"Seamlessly. Easily computed. Going back and then instantly available with zero latency that is supported. But not as well as it will be in terms of covering a variety of kinds of cases. I'm going to be focusing on that area and making sure that we hit that really well because I think that it's going to be the sort of thing that everybody is going to want to be doing, like in a few years. Like everyone's going to want to have real time signals for all their models because why would you want steal signals? Every kind of signal that you're producing. You might as well be able to get it in a really real time way. And once the feature abstraction makes it easy to do, everyone will do that."
51;Speaker 1;00:33:30.140;00:34:13.409;00:00:43.269;"I love that. That's awesome. I think there's almost like a hierarchy of needs from us. I think it's feature stores in particular and I think a lot of people learned this too, because different companies, I'm sure LinkedIn was much more focused on the real time first and all the naming and version and that kind of stuff kind of came with it. It was much more at the same time, a lot of companies I talked to, it's like step one, can we actually have a feature exist as a abstraction? Step two is like, yeah, that feature could be real time. Especially for in case it was easy to do it back. That would be a huge plus. I love the best the direction you guys are going and the space is kind of moving to in general. If we could keep talking all day. I do want to be set up to your time. We will have to bring you back on one of these days."
52;Speaker 2;00:34:13.520;00:34:39.120;00:00:25.600;"Yeah. Thanks a lot, Simba. It's been my pleasure to be here talking to you. This has been a great conversation. I'd be happy to talk more. These are exciting times for this work, making it easy for people to use whatever data is available in order to be able to improve the quality of their products using machine learning. It's a really good area to be in. Happy to talk further and hopefully we'll get some other folks here who are interested in what I'm talking about and happy to talk with folks about these things."
