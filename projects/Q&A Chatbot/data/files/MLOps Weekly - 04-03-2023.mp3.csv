Number;Speaker;Start time;End time;Duration;Text
0;Simba Khadder;00:00:06.170;00:00:22.730;00:00:16.560;"Hey everyone. Simba Khadder here and you are listening to the MLOps Weekly Podcast. Today we're doing something a little bit different. I'm going to be speaking with our head of MLOps here on Featureform, Mikiko. Before I get further Mikiko, why don't you go ahead and introduce yourself."
1;Mikiko Bazeley;00:00:22.930;00:00:51.720;00:00:28.790;"Hey, everyone, my name is Mikiko Bazeley and I joined Featureform pretty recently, let's say around last October, as head of MLOps as Simba mentioned. Prior to joining Featureform, I've worked a number of roles as a data analyst, data scientist, and even more recently as like an MLOps engineer working on the Mailchimp ML platform team. Was on the Mailchimp platform team."
2;Mikiko Bazeley;00:00:51.780;00:01:16.460;00:00:24.680;"I've also worked in a ton of different industries, like you name it, real estate tech, HR tech, antipiracy, which yeah, I know, I feel bad about that, but a girl's got to pay rent, solar, even for example 3D design modeling, which has become more relevant for a company like Autodesk. Really super happy to finally have this conversation on the podcast."
3;Simba Khadder;00:01:16.610;00:01:43.330;00:00:26.720;"I know we've been talking about doing this forever, so I'm really excited to be able to have you on. Man, there's so much to talk about. I would love to start maybe by talking about you started your career on a sergeant career, but you went and made the jump from more applied data scientist to more ML engineer. I would love to first jump into that transition, that difference that you saw between being a data scientist and ML engineer. Yeah, let's just start there."
4;Mikiko Bazeley;00:01:43.430;00:01:58.780;00:00:15.350;"Yeah, it's funny because when I've talked to other MLOps engineers or the platform engineer types, everyone's origin story really starts off from the story of trying to move your model from a Jupyter Notebook to something that's actually in production."
5;Mikiko Bazeley;00:01:58.930;00:02:21.830;00:00:22.900;"Specifically that moment came to me early on. Right before I think quarantine hit, I was working as a data scientist focused on growth marketing at Livongo, which was soon to be acquired by this company called Teladoc, which was involved in telemedicine and was considered one of the biggest telemedicine companies in the US at least."
6;Mikiko Bazeley;00:02:22.210;00:02:52.300;00:00:30.090;"I worked for Livongo. Our main focus was really on IoT data for people with chronic conditions, specifically with diabetes, hypertension, and there's a few others. We were just starting to get into kidney failure. Essentially the way we did that was Livongo created a glucose measure or a diabetes monitor. We get that information, analyse it, and then try to make the patients or the users aware that, hey, you might be having a spike."
7;Mikiko Bazeley;00:02:52.860;00:03:10.710;00:00:17.850;"My particular focus, though, at that company was actually trying to get people enrolled into the programs. At Livongo, we would partner with companies like, for example, Home Depot or a few others, where they are either self-insured or even maybe they're part of Medicare Medicaid."
8;Mikiko Bazeley;00:03:10.790;00:03:32.560;00:00:21.770;"My goal was to, number 1, make sure that when the company sent out any emails or even mail, they were actually dealing with mail, make sure that we understood well, first off, whether or not someone was going to sign up and get enrolled into the program. Secondly, what were the potential factors as to why they wouldn't sign up?"
9;Mikiko Bazeley;00:03:34.050;00:03:56.150;00:00:22.100;"There's like a number of things we were trying to understand. We wanted to get people enrolled in these health programs that we really believe would improve their lives, especially with chronic conditions. Of course, in that instance, or in that scenario, what you really want is you want to be able to very quickly segment users or patients who are going to enroll or not enroll."
10;Mikiko Bazeley;00:03:56.570;00:04:27.970;00:00:31.400;"You want to be able to very quickly deliver that to our nurses, to our in service caretakers, etc. They need that information sooner or later. This is especially true if you're trying to look at spikes in their glucose or A1C for diabetes. Being able to have those inferences and predictions served as real-time as possible, whether it's truly real-time or whether it's in fact more of like a batch process, was really important."
11;Mikiko Bazeley;00:04:28.790;00:04:48.630;00:00:19.840;"Of course, when I was first trying to develop predictive models, I had come out of a boot camp prior to that. But the actual deployment and the serving and monitoring was really just not talked about or taught. I'm not blaming the boot camp for it, but I think those are really, really complicated problems."
12;Mikiko Bazeley;00:04:48.700;00:05:08.910;00:00:20.210;"A lot of times we just did not have the either engineering resources or even the experience on the engineering side dealing with machine learning to be able to get those models in production. I was doing a lot of bootstrapping. I saw people around me, even some of the senior data scientists, having that same issue."
13;Mikiko Bazeley;00:05:09.190;00:05:33.760;00:00:24.570;"I realised that, look, this is actually like a real bottleneck for whatever reasons, and it's something that I really want to work on. I feel like it was really valuable because no matter what, I could create the most beautiful models in the world or the most beautiful analyses, but if I couldn't actually get it in front of people in a scalable way, my value as a data scientist was going to be severely bottlenecked and limited."
14;Simba Khadder;00:05:33.910;00:05:46.840;00:00:12.930;"Got it. I guess you started seeing that problem, the ML engineering problem. It sounds like that's what drove you to want to work on that problem in particular, moving to the ML engineering side at Mailchimp."
15;Mikiko Bazeley;00:05:46.930;00:06:10.270;00:00:23.340;"Yeah, absolutely. A huge part of it, too, was I noticed that early on there was this gap between data science practitioners or people who are building these data science or machine learning assets and the deployment and the scaling layer. How do we standardise and how do we make this into a practice that can really be enabled?"
16;Mikiko Bazeley;00:06:10.730;00:06:36.830;00:00:26.100;"I was super interested in that, especially since I was like boot camp grad, self-taught and formally taught. I wanted to really understand what are ways that we can enable experimentation and innovation at scale. Yes, that's when I started saying, okay, I want to pivot more to the platform side, start to make those moves, and then join the Mailchimp team to do exactly that work."
17;Simba Khadder;00:06:36.930;00:07:03.350;00:00:26.420;"You talked about value both on the data science side and the MLOps side. On the data science side, you mentioned the MLOps was bottlenecking your ability to create value as a data scientist. I want to first go into that piece. As the macro environment is changing, people especially like data scientists, I think many of them are looking around and trying to say hey, how am I bringing value to my organisation? And trying to be able to show that."
18;Simba Khadder;00:07:03.940;00:07:12.150;00:00:08.210;"My first question to you is, how does a data scientist do that? Where do you feel like data scientists provide value to an organisation?"
19;Mikiko Bazeley;00:07:12.890;00:07:56.990;00:00:44.100;"Yeah, and actually, I would argue that what I was being bald necked on was a lack of MLOps practice and tooling. I would say that was actually the bigger bottleneck. In terms of where data scientists bring value in an organisation, so I remember a few years ago and it's funny how stuff just changes, what people think is really important in a data scientist role. A few years ago, everyone was all about the full stack data scientists, similar to how they're all about the full stack, like web unicorn or what have you. Everyone was saying, look, data scientists should be able to do infrastructure. They should be able to do Kubernetes and Jenkins and Terraform, etc."
20;Mikiko Bazeley;00:07:57.960;00:08:23.150;00:00:25.190;"I really don't think that's where data scientists excel. For me, personally, I was very blessed in being able to see really awesome data scientists at Teldoc, who created some amazing analyses and research on what were some of the risk factors for our chronic condition population with regards to COVID. There was some fantastic work there."
21;Mikiko Bazeley;00:08:23.220;00:08:48.190;00:00:24.970;"I also saw some really awesome data scientists at Mailchimp, some of the really awesome projects that they were doing. Even before, I'm trying to think. This was maybe around the time stable diffusion came out and I think a little bit after, and maybe right before ChatGPT-3 was being announced, or sorry, GPT-3 was being announced, not ChatGBT, but GPT-3 was being announced."
22;Mikiko Bazeley;00:08:48.270;00:09:02.840;00:00:14.570;"Right in between that time period, for example, some of our data scientists, they were actually trying to figure out ways to create business value using some of the existing generative AI models out there for small, medium-sized business users Mailchimp."
23;Mikiko Bazeley;00:09:03.680;00:09:33.710;00:00:30.030;"I remember there's a very specific set of models that they were working on to power what's called Creative Assistant. Specifically what Creative Assistant does is in order to help small, medium-sized business owners relieve the burden of, for example, having to create email content like the images, the copy, rather than having to go hire and organise like a designer off of Fiverr to do that."
24;Mikiko Bazeley;00:09:33.830;00:10:05.170;00:00:31.340;"Instead, what someone could do was they could use Creative Assistant to scrape their own website or business or to get all the design elements out of their existing site or storefront with Mailchimp. They could then create pretty easily like in a UI, generate different email layouts, they could generate different social media layouts. They could generate different photographic assets like color, copy, from a number of the machine learning pipelines that were running."
25;Mikiko Bazeley;00:10:06.530;00:10:29.030;00:00:22.500;"I thought that was awesome work. The team at Mailchimp, they were proving business value with cutting edge ML. A huge part of why they were able to do that was one, they had a very strong understanding of ML systems, for example, ML systems, ML algorithms, specifically."
26;Mikiko Bazeley;00:10:29.140;00:10:42.440;00:00:13.300;"For example, how would you design a recommendation pipeline? What are the best algorithms for certain tasks? How do you measure those algorithms? What are some things you need to look for and worry about in the data?"
27;Mikiko Bazeley;00:10:42.580;00:11:00.540;00:00:17.960;"As well as like, for example, being able to interface with the product teams, being able to interface with marketing. I think there's a lot of things that data scientists can do. Largely it's adding the creative innovation and the awareness of research that's going on."
28;Mikiko Bazeley;00:11:00.680;00:11:14.340;00:00:13.660;"I just don't feel like infrastructure is where they should be spending their time or even figuring out what the happy path is for getting malls deployed. They should really be spending time on experimentation and training."
29;Simba Khadder;00:11:14.500;00:11:34.060;00:00:19.560;"Got it. Just to repeat back to you what I think I heard, it almost sounds like what you're saying is most of the data scientists values, it's in doing data science. It's in taking data and finding insights from it or even feeding those insights back into product features like you mentioned Creative Assistant. Is that fair? Is that how you should think about it?"
30;Mikiko Bazeley;00:11:34.510;00:11:35.660;00:00:01.150;"Yeah, absolutely."
31;Simba Khadder;00:11:36.190;00:11:52.530;00:00:16.340;"What's the difference? I know you were talking about how you were working on the growth side. On the growth side, it seemed like it'd be more internal facing. There's also kind of this external facing, I think people call it like a product data scientist now, where you're building like a recommender system is a very obvious example of this."
32;Simba Khadder;00:11:52.630;00:12:00.160;00:00:07.530;"Do you feel like there's these different classes of data science, or do you feel like it's all one umbrella and that any data scientist can jump between them?"
33;Mikiko Bazeley;00:12:00.630;00:12:31.400;00:00:31.100;"I feel like when there is specialisation, it's more about domain as opposed to external internal. I do feel like this external internal thing, I see it and I do feel like that's almost like a maladaptive practice where it should be about... Where a data scientist should be specialising is in maybe domain and the types of problems that they're equipped to work on as opposed to whether they're working on stuff that's internal versus external."
34;Mikiko Bazeley;00:12:31.470;00:12:58.160;00:00:26.690;"I think that's for the platform engineers to be able to enable data scientists, like in a company to create models regardless of where the ultimate outcome or the end goal is. I'm curious. I feel like I see data scientists when they do eventually specialise, it's like computer vision versus NLP versus forecasting. Do you feel like that's like a fair summarization?"
35;Simba Khadder;00:12:58.330;00:13:27.230;00:00:28.900;"I've actually never thought of it this way, but I actually think you're spot on. Why I haven't thought of it this way is my background was much more recommender systems, which are almost always external. These have a wide variety of problems that just don't exist in what we were calling internal problems. There's some overlap, but I just think that recommended systems is a specific specialisation that typically just happens to be tied to what we're calling external."
36;Simba Khadder;00:13:27.340;00:13:50.020;00:00:22.680;"But if you think about bureau vision, there are many use case I could imagine. Let's say you're having or let's say you're doing NLP like document processing, you could be doing document processing because you are providing people I know like autocorrect on their email or because you are trying to do analytics for an internal team."
37;Simba Khadder;00:13:50.830;00:14:10.140;00:00:19.310;"I feel like as a data scientist, assuming that the MLS platform was very strong, most of the techniques you're going to be using are the same. The things that are different is like the scale and other things which again in theory are abstracted away by the MLS platform. An ML platform you're using."
38;Simba Khadder;00:14:10.220;00:14:28.660;00:00:18.440;"I think that's right. I feel like as a data scientist, you might become specialised in the type of data, the type of model, the type of problem space you're working in. And hey, I'm doing a million things then external or hey, I'm like outputting a spreadsheet almost shouldn't matter."
39;Simba Khadder;00:14:29.270;00:14:58.280;00:00:29.010;"If you're truly trying to extract the pure value a data scientist can provide, but no one else can provide, you want to just get them as close as possible to driving insights, building models. Everything else is a detail and all the other things about external and internal. Thou they do exist, those are just more maybe artifacts of just the fact that we're so naive in how we build our ML platforms today. It's just there's so much to be done."
40;Simba Khadder;00:15:00.190;00:15:18.990;00:00:19.200;"We could almost draw the same analogy in building, let's say a typical dev service. If you're building an internal dev service, chances are the scale is way lower. DUI doesn't have to be as good. There's all this stuff that doesn't matter as much if it is internal as opposed to if you're building..."
41;Simba Khadder;00:15:19.190;00:15:37.380;00:00:18.190;"At the external, there's this whole new problem space. But as time has gone on, sure we can ignore more things if we're building internal and external just based on scale. But I mean, nowadays it's all like Kubernetes anyway, they're all like services anyway, they're all in docker, it's all written in the same languages."
42;Simba Khadder;00:15:37.550;00:15:48.300;00:00:10.750;"Over time, it's almost we've gotten rid of that differentiation. The only difference is that your requirement space might be slightly different, but that's really the only place that comes up."
43;Mikiko Bazeley;00:15:48.750;00:16:06.530;00:00:17.780;"Yeah, it's interesting because if you think about it like a platform or an ML stack that's done well, the data science shouldn't even have to think about the implementation details other than their work on the training, experimentation, and a good chunk of the data side."
44;Mikiko Bazeley;00:16:06.680;00:16:33.430;00:00:26.750;"Yet I feel I've definitely seen teams where for whatever reason, they just get stuck in the tooling or the infrastructure of the platform and then they essentially have to relearn really bad or atypical patterns. That might seem very intuitive to pure engineers, but from a data science perspective, some patterns are just absolutely painful."
45;Simba Khadder;00:16:33.590;00:16:54.870;00:00:21.280;"Yeah, I think there's a lot of differences between data scientists and engineers. Data science is inherently there's no clear path. It's not like every iteration you do is better or closer to where you're going to end up. Where in software engineering, typically, you know where you're going. You're moving towards solving this requirement set."
46;Simba Khadder;00:16:55.010;00:17:17.410;00:00:22.400;"With data science, it's a lot more of a windy path and a lot more like, experimentation doesn't really exist in software engineering, not in the same way. You might experiment on product features, you run tests, but you don't really experiment in the sense of like, I'm just going to try this giant approach and then, let's just throw it away and try this approach."
47;Simba Khadder;00:17:17.490;00:17:32.930;00:00:15.440;"It just doesn't happen. You don't throw things away. Throw away 99% of what you do in software engineering, that would be awful. Where in data science that would be pretty normal if you throw away most of what you did 'cause it's all about learning more so that you can eventually build the best thing."
48;Simba Khadder;00:17:33.030;00:17:54.510;00:00:21.480;"But yeah, actually going into the ML Stack, we did talk about these patterns are different and I think we've both talked in depth before about how the ML stack and ML platform should really be focused on the data scientist fair to end user. If they hate it, then you aren't doing your job as the ML platform team."
49;Simba Khadder;00:17:54.630;00:18:07.970;00:00:13.340;"But maybe just to even broaden that question a bit for you, what do you think is the goal of an ML platform? What are the key metrics or maybe even the North Star of an ML platform?"
50;Mikiko Bazeley;00:18:08.170;00:18:29.690;00:00:21.520;"Yeah, totally. I think something that interests me, and I'd be curious to hear your thoughts about this later is, how often it feels like ML platforms are almost not treated seriously as platforms or as even the concept of platform as a product, like how often ML platforms are not treated as products."
51;Mikiko Bazeley;00:18:29.810;00:18:51.220;00:00:21.410;"At least I've worked on all different maturity layers of the ML stack from trying to create an ML stack, which was fun for a very early stage real estate tech startup, which was hard. That was where I found out that you in fact cannot write a production data pipeline using pandas and that is a very bad way to go."
52;Mikiko Bazeley;00:18:52.490;00:19:12.080;00:00:19.590;"Early stage to late stage, like for example, Mailchimp, I thought there's like a few pieces that were a little bit missing, like that bridging between the dev training experimentation to the serving part was a little bit rough, but I think they had 80%-90% the way there."
53;Mikiko Bazeley;00:19:12.160;00:19:41.560;00:00:29.400;"In terms of North Star metrics, the thing that is fascinating is for I guess people who aren't aware, North Star metrics and there's also the one metric that matters. The idea of a North Star metric was a single metric that everyone in a company or business could rally around including marketing, sales, revenue, product, to keep the boat going forward."
54;Mikiko Bazeley;00:19:42.090;00:20:04.600;00:00:22.510;"I feel like we do a very bad job of measuring ROI or even efficacy when it comes to ML platforms. Like for example, there is a fun conversation, actually there's been a few conversations right in the MLS community, but also in other discords I've seen where people are trying to figure out what metric to measure."
55;Mikiko Bazeley;00:20:04.750;00:20:36.470;00:00:31.720;"Should it be time to deploy? Should it be deployment frequency? Should it be change failure rate? Should it be number of manual tasks or what have you? I think a huge part of it comes down to we like to conflate project metrics with platform metrics. Very specifically, a lot of times platform teams, they like to try to measure their velocity through how long it takes projects to get up and running."
56;Mikiko Bazeley;00:20:36.620;00:20:58.050;00:00:21.430;"That's challenging for a number of reasons. One, data science projects can kind of go sideways early on. Because there's always this big cloud question mark, I think it's really hard to peg like, we're going to peg the efficacy of our platform to how well the data science pod is getting their project through."
57;Mikiko Bazeley;00:20:58.390;00:21:31.450;00:00:33.060;"Then the second part is a lot of platform teams, they focus on too many metrics that are not tied to the direct behavior that they're trying to measure. They're measuring all these baseline metrics that won't even tell you, for example, like deployment frequency to some degree that's related to the size of your data science team. Same with meantime to restore could be an interesting one, but at the same time, ideally, your models aren't breaking."
58;Mikiko Bazeley;00:21:32.450;00:21:59.830;00:00:27.380;"I think there's a couple of areas where we could streamline our understanding of metrics. I don't necessarily feel like the DORA, SRE platform metrics of time to restore, change failure rate, lead time for change, deployment frequency, I don't think these are really adequate or they're super relevant for measuring what is a data scientist relationship with an ML platform."
59;Simba Khadder;00:22:00.010;00:22:30.390;00:00:30.380;"Yeah, it's almost like SRE again, like software in general, you write your code and you test it and you expect the business logic to be correct. But in any distributed system, which is where SREs are going to be, there's just so much that can go wrong. It's like you can have a network outage, you can have a partition, you can have weird delays in packets where just like all of a sudden you get these really strange race conditions."
60;Simba Khadder;00:22:30.930;00:22:57.540;00:00:26.610;"I feel like a lot of what SREs focus on is solving those problems where it's just like, hey, there's no way we can write, or it just doesn't make sense to write your code so it's actually perfect. Let's just write it so it's good enough and just let the platform abstract away. Like, yeah, one in every million requests on Netflix is a 404. That's fine. We can accept that. The UI will just refresh it. That's okay."
61;Simba Khadder;00:22:57.700;00:23:09.130;00:00:11.430;"It's better to do that and just handle the error really well than to try to write the code in such a way where it's perfect, because honestly, it can't be. If it's a certain type of network partition just actually can't work."
62;Simba Khadder;00:23:09.230;00:23:30.300;00:00:21.070;"Whereas it sounds like what you're saying, which I really like is, for data science, the problem space is less about like, hey, let's release things quickly and make sure they don't break. It sounds like a lot of the... There's just different stages that just almost don't exist, like experimentation and even just like data analysis."
63;Simba Khadder;00:23:31.330;00:23:50.520;00:00:20.130;"The types of things that can break, it's almost like things don't break in a very binary way, too. Like a model might just drift and then all of a sudden you end up with like, hey, we just might want to retrain or we might want to change our training set in such a way where it better encapsulates the behavior we're seeing in production today."
64;Simba Khadder;00:23:51.180;00:24:12.780;00:00:21.600;"You mentioned metrics that aren't great or maybe are imperfect. Is there such a thing as a set of metrics or even just a single North Star metric for an ML platform team, if you had to say one? Or do you think that in general, what does it depend on? If I'm running an ML platform team and I'm trying to decide, hey, how do we measure our efficacy, what do I do?"
65;Mikiko Bazeley;00:24:12.920;00:24:31.340;00:00:18.420;"Yeah, and I'd be curious to hear your thoughts about this afterwards. I do think it depends on the stage of the... It's almost like the maturity of the data science and ml.org and then the stage they are in terms of implementing tooling and platforms."
66;Mikiko Bazeley;00:24:31.490;00:24:54.540;00:00:23.050;"Because for example, what's really interesting is that so much of the conversation about Stacks has really been driven by the Google scale companies. But I think what we're seeing, for example, even Featureform, is there are so many companies that want to build ML platforms that just don't even fit. They're not the big tech co-companies."
67;Mikiko Bazeley;00:24:54.700;00:25:12.230;00:00:17.530;"Some of them are startups, some of them are mid size. Some of them, for example, they've maybe only deployed a few models and some of them have deployed like hundreds of models. For some folks, they have one data scientist who's also the data person. For other teams they have like 10-20 data science."
68;Mikiko Bazeley;00:25:13.000;00:25:31.890;00:00:18.890;"I feel like there's so many different requirements on stacks. I don't think there's one single metric that's ever going to be right for all the stages. But I think if we're thinking more about platforms as products, we should be thinking, for example, about how would you measure a product, how would you measure adoption and engagement?"
69;Mikiko Bazeley;00:25:31.970;00:25:55.030;00:00:23.060;"Reliability metrics are great, but really if you are trying to develop a platform that people love, I mean, sometimes also I think the metric could just be like how many people are using it. But I'd be curious, what do you think about the different flavors and maturities of stacks that are out there? Is there a single way to design and build a stack?"
70;Simba Khadder;00:25:55.160;00:26:16.180;00:00:21.020;"Yeah, there's always two sets of problems that exist. One is what I would call the people organisation problems, which is like getting a group of data scientists to work together productively or even in themselves. It's like being productive alone and organised and giving them the tools they need to do that."
71;Simba Khadder;00:26:17.130;00:26:44.580;00:00:27.450;"Then there is almost the infrastructure problems where it's things like, I need to hit this level of latency, I need to be able to handle this much data. I need to be able... It's usually very binary. Those look like more traditional, I guess, like North Star KPIs, like metrics. Like, hey, we need to be able to handle this much data and this latency, this P99 latency."
72;Simba Khadder;00:26:45.010;00:27:19.520;00:00:34.510;"Those are great. Depending on type of company, whether big or small, you might have that. It just really depends. If you're a SaaS company, even though you might have a lot of revenue and a lot of employees, a lot of data scientists, chances are your amount of data is dwarfed by even a much smaller B2C company or fintech company, which just typically have way, way more data way earlier on. You actually can see that in the fact that they have way more data scientists per headcount as a ratio, even like early on, they'll have big data science teams."
73;Simba Khadder;00:27:20.510;00:27:40.050;00:00:19.540;"That's the first set of problems which I think there is a place for it and it's like the Google scale thing is a good way to think of it, but it's almost not necessarily how big of a company you are, it's almost like scale of data and where are you deployed. If you're doing real time recommendations on a ton of users nowadays, you can be a small company and still have a problem."
74;Simba Khadder;00:27:40.390;00:28:14.090;00:00:33.700;"Then the other set of problems which are like the organisational problems are very much going to be correlated to how big a team you are. Typically also there's other aspects like how regulated are you? If you're in banking, you probably have a lot more regulation, and you'd be much more likely to make your data science team less productive to make sure that they were not going against regulation and vice versa. In the sense of like, I'd rather make sure that we are correct of our regulation, even if it requires a few more steps from data scientists than to risk them self managing it."
75;Simba Khadder;00:28:14.430;00:28:28.630;00:00:14.470;"That's one piece, and I think that piece looks very similar to the SRE type metrics. The other types of metrics which I think are way more important typically and way less understood are the exact ones you're talking about, which are the product metrics."
76;Simba Khadder;00:28:28.760;00:28:50.570;00:00:21.810;"Engagement is obviously a good one here. I mean, I would be very surprised if any MLOps platform lead has ever run an NPS score on their platform of [inaudible 00:28:41] data scientists. It would be super interesting. You would learn so much. Just go ask your data scientist like you like our platform, why do you like it? Why do you not like it?"
77;Simba Khadder;00:28:50.670;00:29:04.370;00:00:13.700;"It's almost like the age old startup like, have you talked to your users? It's like, no. It's like, okay, well, maybe start there. Then you'll probably learn a million things you didn't know before just by talking to your users."
78;Simba Khadder;00:29:04.500;00:29:25.490;00:00:20.990;"We have funny stories where we talked to a data scientist who was using Featureform. We're looking at using Featureform and they're like, yeah, we don't have a feature store. Then I had one situation where this happened and then a month later the person emails me and says, yeah, I was starting to use it more and then I was notified that we actually have an internal feature store."
79;Simba Khadder;00:29:25.990;00:29:40.690;00:00:14.700;"Then from my perspective, it was so funny because it was about a huge data science team. I'm like, wait, so there's a whole platform team building a feature store, but one of the dozens of data scientists are completely unaware that all this stuff exists internally."
80;Simba Khadder;00:29:40.850;00:29:53.150;00:00:12.300;"It just shows that the platform team is building for sake of building, and maybe focus on these other metrics of what's our latency, what's our uptime, and not really worry about, hey, are we actually being used? Do people like using it? Are people going to use it?"
81;Simba Khadder;00:29:53.170;00:30:12.460;00:00:19.290;"'Cause if you don't have that, it's not going to stick and you're going to end up with like 45 different AML platforms, which I've seen some of the large banks where they have dozens of different ammo platforms, and there's never really one standard rule in law 'cause they all have their own problems. None of them, I think, ran in that product-focused mindset."
82;Mikiko Bazeley;00:30:12.630;00:30:37.430;00:00:24.800;"Yeah, it's really fascinating how enablement is always like the last mile of an ML platform tool that just no one ever wants to do, even as simple as making sure you have a clear, centralised documentation for where you can find things. For example, like, what were the models that were trained? What features are being used within models? It's really fascinating how hard some of those problems are of just making stuff usable."
83;Mikiko Bazeley;00:30:38.040;00:31:09.200;00:00:31.160;"Something to me that's really fascinating and I feel like so many people, especially when they look at the most recent MAD landscape of 2023, just feel this absolute sense of chaos looking at that map. To me, what's really fascinating is if we were to go back and look at all the tools that are on there, I would be so curious about that NPS score, like the end users of the tools and all that."
84;Mikiko Bazeley;00:31:09.340;00:31:28.390;00:00:19.050;"What's the NPS score of, for example, someone who's using a data orchestration tool or a training or mall registry tool or what have you, because the current landscape, it just lays out everything, but not all those tools are equal."
85;Mikiko Bazeley;00:31:28.500;00:31:47.240;00:00:18.740;"It's fascinating because I think when companies and teams are looking at risk and tool adoption, they'll look at all those tools as being the exact same when there's actually a lot of differences between those tools and the implication on which ones are best for which stacks or use cases."
86;Simba Khadder;00:31:47.390;00:32:06.990;00:00:19.600;"Yeah, I bet if you take the MAD landscape, show it to, let's call it 1,000 data scientists, ask them to point out or circle the tools that they love. You would take the MAD landscape and probably drop it down to like 15 products or something. It's really very part..."
87;Simba Khadder;00:32:07.030;00:32:25.010;00:00:17.980;"I mean, we're so early. It's not like there's a lot of products that are finding their way and that have a ton of potential. But I do think that especially if you count internal products, the right metric for a lot of companies, once you check off, the metrics about latency and all those are like checkoff."
88;Simba Khadder;00:32:27.250;00:32:41.910;00:00:15.800;"You need four nines of uptime. Do we have that yes or no? If no, like, solve? If yes, then you're good. Chances are... I mean, it depends, but in a lot of these situations, it's binary given latency."
89;Simba Khadder;00:32:42.150;00:33:03.910;00:00:21.610;"You can make it better, but let's say you have a recommender system and you need to serve a feature in under eight milliseconds. Let's say you were at eight milliseconds or seven milliseconds, and you drop it to five. Well, if it takes the page a certain amount of time to load anyway, you haven't really made anything better by making that metric better."
90;Simba Khadder;00:33:03.600;00:33:18.920;00:00:15.010;"A lot of these metrics are also very binary, but a metric that is not binary, and it's surely a good one to focus on if you're building or leading the ML platform team is NPS and engagement, just like, do people like using this thing?"
91;Simba Khadder;00:33:18.980;00:33:39.360;00:00:20.380;"Obviously, it needs to solve all the other problems, but the UX is, in my opinion, the hardest part to solve. That's why products like Terraform and you could name a lot of... Snowflake is another good example. Why is Snowflake such a big company? Well, they just really figured out NPS. People love using Snowflake because it just works and it's really simple."
92;Simba Khadder;00:33:40.150;00:33:59.340;00:00:19.190;"I feel like the closer MLOps platforms can move to that, a lot of times they want to add that new feature that bell and whistle, and a lot of times it's just like, make it easier, make it simpler. Just solve the problem so that you check off all these binary check marks. But as a data scientist, it just feels natural. It fits my flow, it fits my way of thinking."
93;Mikiko Bazeley;00:33:59.500;00:34:17.790;00:00:18.290;"Yeah. I think that's one of the reasons why I was so excited to join Featureform and to work with everyone here is because I feel like in some ways, MLOps is failing the original end users of MLOps, which is the data scientists."
94;Mikiko Bazeley;00:34:18.370;00:34:33.810;00:00:15.440;"We should be making it really easy for them to do the right things. I feel like as an ecosystem, it's questionable how much we've really succeeded with that. I think asking data scientists to learn infrastructure was maybe not the right direction."
95;Mikiko Bazeley;00:34:33.920;00:34:58.630;00:00:24.710;"I don't know. I'm super excited for the new cycle of MLOps tools, both in the orchestration and workflows in terms of getting back to that state of making it easy and seamless for data scientists to do the right things to make themselves more productive. Also the platform engineers are pulling their hair out a little bit less."
96;Simba Khadder;00:34:58.830;00:35:27.810;00:00:28.980;"I think one thing that's great that's happening is with the hype around MLOps, like the bad type of the hype around MLOps finally starting to fall out, all that's left is value creation. Anyone who's still building MLOps and still cares about MLOps is not doing it because there's a ton of VC money or there's another unicorn every month. Because if you want to be doing that, you'd be doing during the VI now."
97;Simba Khadder;00:35:27.930;00:35:47.160;00:00:19.230;"If you're still doing boring MLOps, it's because you actually care about the problem. If you actually care about the problem, I think what we're seeing a lot of now is people finally doing a lot of what we're talking about and treating their MLS platform as a product and not as this weird low level infrared tool."
98;Simba Khadder;00:35:47.570;00:36:12.060;00:00:24.840;"It's a product that data scientists use and I'm very excited for it. I think that I'm excited for us as an ecosystem beyond just like Featureform to actually start to make data scientists lives easier and to get to this point where it's almost like a duh, like CI/CD. There was a time where it was like, that's like crazy that you do that, and nowadays it's like crazy if you don't."
99;Simba Khadder;00:36:12.330;00:36:21.270;00:00:09.140;"I think we're going to see the same in ML platforms. It becomes so easy, it becomes so part of the workflow that everyone just uses it because that's just how you do data science."
100;Simba Khadder;00:36:21.660;00:36:36.940;00:00:15.280;"When you do a boot camp, it's not like the MLOps chapter is like a whole never complicated chapter. It just fits in with the original things, like the way you do it from the beginning. This inherently is built to be scaled, to be reliable, to easily fit into a platform."
101;Simba Khadder;00:36:37.100;00:36:44.710;00:00:07.610;"We already do that with Docker, we do that with a lot of tools and traditional software engineering. I think we're going to see the same thing in data science."
102;Simba Khadder;00:36:44.760;00:36:57.850;00:00:13.090;"Mikiko I feel like we could talk all day and we probably will continue to talk after this, but I do know we're at time and I just want to thank you for making the time, hopping on and chatting with me for everyone to listen to."
103;Mikiko Bazeley;00:36:57.970;00:37:03.730;00:00:05.760;"Yeah, this is great. Luckily, since I'm part of the team now, hopefully we'll have future conversations, too."
104;Simba Khadder;00:37:03.830;00:37:06.740;00:00:02.910;"I think we will. I think we will. Awesome. Thank you."
